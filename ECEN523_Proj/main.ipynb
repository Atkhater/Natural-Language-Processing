{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2046,
     "status": "ok",
     "timestamp": 1718413418506,
     "user": {
      "displayName": "marmarlife",
      "userId": "07574260493131904559"
     },
     "user_tz": 420
    },
    "id": "Ud8baS5lWthI",
    "outputId": "3e0c64c6-0145-433d-8bb8-a9e044a1ae4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DLJUlimgkV39"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install faiss-cpu\n",
    "!pip install fireworks-ai\n",
    "!pip install openai==0.28.0\n",
    "!pip install PyPDF2\n",
    "!pip install sentence-transformers\n",
    "import faiss\n",
    "import fireworks.client\n",
    "import numpy as np\n",
    "import openai\n",
    "import os\n",
    "from PyPDF2 import PdfReader\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lHsECjlYoUJN"
   },
   "outputs": [],
   "source": [
    "# API Keys used throughout the code\n",
<<<<<<< HEAD
    "fireworks.client.api_key = #insert fireworks key here\n",
    "openai.api_key = #insert openai key here"
=======
    "#Insert fireworks and openai keys below"
>>>>>>> 2ca89d8 (Removed API keys)
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "executionInfo": {
     "elapsed": 964,
     "status": "ok",
     "timestamp": 1718413443025,
     "user": {
      "displayName": "marmarlife",
      "userId": "07574260493131904559"
     },
     "user_tz": 420
    },
    "id": "Lh9kWmxox1DV",
    "outputId": "8b77d464-b379-4abc-888f-79a5894d54e8"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' they can learn from large amounts of data and make predictions or decisions with high accuracy. However, these models require a lot of computational power and time to train, which can be a bottleneck for many applications.\\n\\nOne way to address'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_completion(prompt, model=\"llama-v2-7b\", max_tokens=50):\n",
    "    model_with_dir = \"accounts/fireworks/models/\" + model\n",
    "    completion = fireworks.client.Completion.create(\n",
    "        model=model_with_dir,\n",
    "        prompt=prompt,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=0\n",
    "    )\n",
    "    return completion.choices[0].text\n",
    "\n",
    "get_completion(\"Deep Learning models are advantageous because\", model=\"mistral-7b-instruct-4k\")  # Appears to be superior to llama2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18598,
     "status": "ok",
     "timestamp": 1718413461603,
     "user": {
      "displayName": "marmarlife",
      "userId": "07574260493131904559"
     },
     "user_tz": 420
    },
    "id": "2cVSKsl_EO2D",
    "outputId": "ff673e51-69b3-40a8-f89b-a8aa6c47833d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: Redmon_You_Only_Look_CVPR_2016_paper.pdf ...\n",
      "Loading: 1512.03385v1.pdf ...\n",
      "Loading: 1505.04597v1.pdf ...\n",
      "Loading: NIPS-2015-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks-Paper.pdf ...\n",
      "Loading: Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf ...\n",
      "Loading: ioffe15.pdf ...\n",
      "Loading: 1412.6980v9.pdf ...\n",
      "Loading: 1409.0473v7.pdf ...\n",
      "Loading: NIPS-2014-sequence-to-sequence-learning-with-neural-networks-Paper.pdf ...\n",
      "Loading: srivastava14a.pdf ...\n",
      "Loading: NIPS-2014-generative-adversarial-nets-Paper.pdf ...\n",
      "Loading: 1312.6114v11.pdf ...\n",
      "Loading: 1301.3781v3.pdf ...\n",
      "Loading: NIPS-2012-imagenet-classification-with-deep-convolutional-neural-networks-Paper.pdf ...\n",
      "Loading: vandermaaten08a.pdf ...\n",
      "Loading: blei03a.pdf ...\n",
      "Loading: A_1010933404324.pdf ...\n",
      "Loading: BF00058655.pdf ...\n",
      "Loading: s41586-021-03819-2.pdf ...\n",
      "Loading: 2010.11929v2.pdf ...\n",
      "Loading: NeurIPS-2020-language-models-are-few-shot-learners-Paper.pdf ...\n",
      "Loading: 1810.04805v2.pdf ...\n",
      "Loading: NIPS-2017-attention-is-all-you-need-Paper.pdf ...\n"
     ]
    }
   ],
   "source": [
    "# Make sure to have a direct link from MyDrive to the shared folder\n",
    "\n",
    "def load_pdf_documents_from_folder(folder_path):\n",
    "    documents = []\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith('.pdf'):\n",
    "            print(f\"Loading: {file_name} ...\")\n",
    "            pdf_path = os.path.join(folder_path, file_name)\n",
    "            with open(pdf_path, 'rb') as pdf_file:\n",
    "                text = \"\"\n",
    "                pdf_reader = PdfReader(pdf_file)\n",
    "                for page_num in range(len(pdf_reader.pages)):\n",
    "                    page = pdf_reader.pages[page_num]\n",
    "                    text += page.extract_text()\n",
    "                    documents.append(text)\n",
    "    return documents\n",
    "\n",
    "background_documents = load_pdf_documents_from_folder('/content/drive/MyDrive/ECEN523_Proj/Background')\n",
    "references_documents = load_pdf_documents_from_folder('/content/drive/MyDrive/ECEN523_Proj/References')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13913,
     "status": "ok",
     "timestamp": 1718413475494,
     "user": {
      "displayName": "marmarlife",
      "userId": "07574260493131904559"
     },
     "user_tz": 420
    },
    "id": "zPyZRGWSWkpw",
    "outputId": "96435d6d-84d9-4bfa-cd26-6b7507a07b48"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Background pages embedded: 273\n",
      "Number of References pages embedded: 86\n"
     ]
    }
   ],
   "source": [
    "# Embeddings done for the background and references\n",
    "\n",
    "background_embeddings = SentenceTransformer('all-mpnet-base-v2').encode(background_documents)\n",
    "references_embeddings = SentenceTransformer('all-mpnet-base-v2').encode(references_documents)\n",
    "\n",
    "print(f\"Number of Background pages embedded: {len(background_embeddings)}\")\n",
    "print(f\"Number of References pages embedded: {len(references_embeddings)}\")\n",
    "\n",
    "background_index = faiss.IndexFlatIP(background_embeddings.shape[1])\n",
    "references_index = faiss.IndexFlatIP(references_embeddings.shape[1])\n",
    "\n",
    "background_index.add(background_embeddings)\n",
    "references_index.add(references_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QlY5vKpmKW1B"
   },
   "outputs": [],
   "source": [
    "# Generate response using OpenAI API\n",
    "def generate_response(query, passages):\n",
    "    concatenated_passages = '\\n'.join([' '.join(sublist) for sublist in passages])[:2000]\n",
    "    prompt = f\"{query} context: {concatenated_passages}\"\n",
    "    # Utilizing GPT-3.5 to model our responses.\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        max_tokens=150,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Retrieves and returns top_k number of passages\n",
    "def retrieve_passages(query, index, documents, embedding_model='all-mpnet-base-v2', top_k=3):\n",
    "    query_embedding = SentenceTransformer(embedding_model).encode([query])[0]\n",
    "    _, idx = index.search(np.array([query_embedding]), top_k)\n",
    "    passages = []\n",
    "    for i in idx[0]:\n",
    "        passages.append(documents[i])\n",
    "    return passages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 79052,
     "status": "ok",
     "timestamp": 1718414401844,
     "user": {
      "displayName": "marmarlife",
      "userId": "07574260493131904559"
     },
     "user_tz": 420
    },
    "id": "MnzNutujXabT",
    "outputId": "2e7088ae-fe68-4e0b-a854-8e56eede4d05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome! Enter 'q' to quit\n",
      "User:hi\n",
      "\n",
      "Hello! It looks like you have provided some information related to a research paper on word representations in vector space. If you have any specific questions or need assistance with understanding the content or any related information, feel free to ask!\n",
      "User:Can you give me 2 machine learning techniques?\n",
      "\n",
      "Two machine learning techniques that are commonly used in natural language processing tasks like word representation learning are:\n",
      "\n",
      "1. **Word2Vec**: Word2Vec is a popular technique for learning word embeddings in vector space. It uses neural networks to learn distributed representations of words in a continuous vector space where similar words are close to each other. Word2Vec models, such as Continuous Bag of Words (CBOW) and Skip-gram, have been widely used for generating word embeddings.\n",
      "\n",
      "2. **GloVe (Global Vectors for Word Representation)**: GloVe is another technique for learning word embeddings that focuses on the global statistics of the corpus to learn word vectors. It leverages co-occurrence statistics of words in the corpus to capture semantic relationships between\n",
      "User:I want to write a research paper. For my introductory paragraph, what should my topic sentence be for machine learning techniques?\n",
      "\n",
      "The topic sentence for your introductory paragraph on machine learning techniques could be:\n",
      "\n",
      "\"In recent years, significant advancements in natural language processing have been driven by the development and application of powerful machine learning techniques, particularly in the realm of word representation learning.\"\n",
      "User:In following that, what should the second paragraph be talking about?\n",
      "\n",
      "The second paragraph could discuss the specific details and applications of latent Dirichlet allocation (LDA) as described in the provided abstract. This could include explaining how LDA is a generative probabilistic model for collections of discrete data, such as text corpora, and how it models items in a collection as mixtures over underlying topics. The paragraph could also touch upon the efficient approximate inference techniques, such as variational methods and the EM algorithm, used in LDA, as well as the results reported in document modeling, text classification, and collaborative filtering compared to other models like the mixture of unigrams model and probabilistic LSI model.\n",
      "User:q\n",
      "Bye!\n"
     ]
    }
   ],
   "source": [
    "# Initializing lists, which can be used to simulate memory\n",
    "bg_passages = []\n",
    "ref_passages = []\n",
    "responses = [\"\"]\n",
    "\n",
    "debug = False\n",
    "print(\"Welcome! Enter 'q' to quit\")\n",
    "\n",
    "while True:\n",
    "    query = input(\"User:\")\n",
    "    if query == \"q\":\n",
    "        break\n",
    "    bg_passages.append(retrieve_passages(responses[-1] + query, background_index, background_documents))\n",
    "    ref_passages.append(retrieve_passages(responses[-1] + query, references_index, references_documents))\n",
    "    responses.append(generate_response(responses[-1] + query, bg_passages[-10:][::-1] + ref_passages[-10:][::-1]))  # Use the ten most-recent pages in each folder\n",
    "\n",
    "    # Log the interaction\n",
    "    print()\n",
    "    if debug:\n",
    "        print(bg_passages[-1])\n",
    "        print(ref_passages[-1])\n",
    "    print(responses[-1])\n",
    "\n",
    "print(\"Bye!\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
