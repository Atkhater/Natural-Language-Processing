{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "\n",
    "In this lab, you will see the power of word embeddings, and how embeddings can be used in different applications.\n",
    "\n",
    "Summarize the papers that were distributed with the module.\n",
    "\n",
    "Efficient Estimation of Word Representations in Vector Space\r\n",
    "\r\n",
    "Paper goes into depth about the representations of words in similarity tests. Introduces 2 model architectures, the Feedforward Neural Net Language Model (NNLM) and Recurrent Neural Net Language Model (RNNLM). The NNLM utilizes 1-to-V coding and hierarchical softmax for its large vocabularies. The RNNLM utilizes a hidden layer for connections and vocabulary. A quick example is using the following words: King - Man + Woman = Queen. They compared their models with a variety of other models to prove that their model is state of the art. They used 5 types of semantic and 9 types of syntactic questions for their word relationship test set. The corpus they used was the Google News training corpus, which contains 6 billion tokens. For the test they used, they restricted it to 1 million most frequently used words. With a simple model architecture, which lowers the computational complexity. \r\n",
    "\r\n",
    "Learning Phrase Representations using RNN Encoderâ€“Decoder for Statistical Machine Translation\r\n",
    "\r\n",
    "Goes into the background of Recurrent Neural Networks (RNN), which consists of a hidden state and an optional output which works on a variable length. Introduces a model RNN Encoder-Decoder, which learns to encode a variable length sequence into a fixed-length vector representation.  It will then decode the fixed-length vector representation back into a variable-length sequence. The encoder is an RNN that reads each symbol of the input sequence, which then updates the hidden state. The decoder is also an RNN which will generate the output sequence by prediction. The experiment was evaluated on the English-to-French translation task of the WMT '14 workshop. They used BLEU scores to compare and show that their SMT system improved. For the Baseline, the BLEU score is 33.30. For CSLM + RNN + WP, the score was 34.64. It was shown that it is effective in learning to propose target phrases and captured semantic and syntactic structures. \r\n",
    "\r\n",
    "Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings\r\n",
    "\r\n",
    "Introduces the paper by showing that embeddings are sexist. Word embeddings capture semantic meanings based on word co-occurrence in text. They also encode social standards. For example, embeddings can associate \"man\" with \"computer programmer\" and \"woman\" with \"homemaker.\" It demonstrates the gender stereotypes for gender based analogies. The goal of the paper was to target these gender biases and debias them. The methods of debiasing are as follows: reduce biasing by ensuring that gender neutral words like nurse are equidistant from man and woman, reduce gender associations with embeddings. For the first test, they evaluated whether the embedding follows a gender stereotype, and evaluated whether the embedding produces analogies that reflect gender stereotypes. The algorithm they used is as follows: Identify gender subspace -> Neutralize and Equalize, or Soften. In the end, the paper demonstrated a methodology of reducing gender biases and stereotypes using these embedding to not accidentally reinforce harmful stereotypes. \r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. GloVe\n",
    "\n",
    "We will first read imdb movie reviews to train a GloVe embeddings.\n",
    "\n",
    "GloVe is computed from a co-occurrence matrix $X$ as follows:\n",
    "\n",
    "$\n",
    "J = \\sum_{i=1,j=1}^{V,V} f(X_{ij}) (w_i^T w_j + b_i + b_j - log(X_{ij}))^2\n",
    "$\n",
    "\n",
    "$f(X_{ij}) = (X_{ij} / X_{\\max})^\\alpha$ if $X_{ij} < X_{\\max}$; otherwise it is $1$.\n",
    "\n",
    "$\n",
    "\\nabla_{w_i} J = f(X_{ij}) w_j (w_i^T w_j + b_i + b_j - log(X_{ij}))\n",
    "$\n",
    "\n",
    "$\n",
    "\\nabla_{w_j} J = f(X_{ij}) w_i (w_i^T w_j + b_i + b_j - log(X_{ij}))\n",
    "$\n",
    "\n",
    "$\n",
    "\\nabla_{b_i} J = \\nabla_{b_j} J = f(X_{ij}) (w_i^T w_j + b_i + b_j - log(X_{ij}))\n",
    "$\n",
    "\n",
    "Explain the intuition behind these equations.\n",
    "\n",
    "**This is a weighted least squares model , with a weighted fuction $f(X_{ij})$ , where the b values are biases, V is the vocabulary.**\r\n",
    "\r\n",
    "**$f(X_{ij})$ One of the class of functions that works well and can be parameterized**\r\n",
    "\r\n",
    "**Equations are used to make sure that not all words are weighted equally, as some of the words are rare or infrequent**\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in c:\\users\\owner\\anaconda3\\envs\\tf\\lib\\site-packages (2.10.0)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\owner\\anaconda3\\envs\\tf\\lib\\site-packages (2.10.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\owner\\anaconda3\\envs\\tf\\lib\\site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\owner\\anaconda3\\envs\\tf\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\owner\\anaconda3\\envs\\tf\\lib\\site-packages (from tensorflow) (2.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\owner\\anaconda3\\envs\\tf\\lib\\site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\owner\\anaconda3\\envs\\tf\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\owner\\anaconda3\\envs\\tf\\lib\\site-packages (from tensorflow) (3.11.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in c:\\users\\owner\\anaconda3\\envs\\tf\\lib\\site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\owner\\anaconda3\\envs\\tf\\lib\\site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\owner\\anaconda3\\envs\\tf\\lib\\site-packages (from tensorflow) (1.26.4)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\owner\\anaconda3\\envs\\tf\\lib\\site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\owner\\anaconda3\\envs\\tf\\lib\\site-packages (from tensorflow) (23.2)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in c:\\users\\owner\\anaconda3\\envs\\tf\\lib\\site-packages (from tensorflow) (3.19.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\owner\\anaconda3\\envs\\tf\\lib\\site-packages (from tensorflow) (69.5.1)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\owner\\anaconda3\\envs\\tf\\lib\\site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\owner\\anaconda3\\envs\\tf\\lib\\site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\owner\\anaconda3\\envs\\tf\\lib\\site-packages (from tensorflow) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\owner\\anaconda3\\envs\\tf\\lib\\site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\owner\\anaconda3\\envs\\tf\\lib\\site-packages (from tensorflow) (0.31.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\owner\\anaconda3\\envs\\tf\\lib\\site-packages (from tensorflow) (1.48.2)\n",
      "Requirement already satisfied: tensorboard<2.11,>=2.10 in c:\\users\\owner\\anaconda3\\envs\\tf\\lib\\site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.11,>=2.10.0 in c:\\users\\owner\\anaconda3\\envs\\tf\\lib\\site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: keras<2.11,>=2.10.0 in c:\\users\\owner\\anaconda3\\envs\\tf\\lib\\site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\owner\\anaconda3\\envs\\tf\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\owner\\anaconda3\\envs\\tf\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (2.29.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\owner\\anaconda3\\envs\\tf\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (0.4.4)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\owner\\anaconda3\\envs\\tf\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\owner\\anaconda3\\envs\\tf\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\owner\\anaconda3\\envs\\tf\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\owner\\anaconda3\\envs\\tf\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\owner\\anaconda3\\envs\\tf\\lib\\site-packages (from tensorboard<2.11,>=2.10->tensorflow) (2.3.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\owner\\anaconda3\\envs\\tf\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\owner\\anaconda3\\envs\\tf\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\owner\\anaconda3\\envs\\tf\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (4.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\owner\\anaconda3\\envs\\tf\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\owner\\anaconda3\\envs\\tf\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\owner\\anaconda3\\envs\\tf\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\owner\\anaconda3\\envs\\tf\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\owner\\anaconda3\\envs\\tf\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\owner\\anaconda3\\envs\\tf\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.11,>=2.10->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\owner\\anaconda3\\envs\\tf\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\owner\\anaconda3\\envs\\tf\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install keras\n",
    "!pip install tensorflow\n",
    "from keras import preprocessing\n",
    "from keras.datasets import imdb\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = 500\n",
    "maxlen = 200\n",
    "emb_size = 16\n",
    "\n",
    "start_char = 1\n",
    "oov_char = 2\n",
    "index_from = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, _), _ = imdb.load_data(\n",
    "    num_words=num_words, maxlen=maxlen, start_char=start_char, oov_char=oov_char, index_from=index_from\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = imdb.get_word_index()\n",
    "inverted_word_index = dict(\n",
    "    (i + index_from, word) for (word, i) in word_index.items()\n",
    ")\n",
    "# Update `inverted_word_index` to include `start_char` and `oov_char`\n",
    "inverted_word_index[start_char] = \"[START]\"\n",
    "inverted_word_index[oov_char] = \"[OOV]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[START] big [OOV] big [OOV] bad music and a [OOV] [OOV] [OOV] these are the [OOV] to best [OOV] this terrible movie i love [OOV] horror movies and i've seen [OOV] but this had got to be on of the worst ever made the plot is [OOV] [OOV] and [OOV] the acting is an [OOV] the script is completely [OOV] the best is the end [OOV] with the [OOV] and how he [OOV] out who the killer is it's just so [OOV] [OOV] written the [OOV] are [OOV] and funny in [OOV] [OOV] the [OOV] is big [OOV] of [OOV] [OOV] men [OOV] those [OOV] [OOV] [OOV] that show off their [OOV] [OOV] that men actually [OOV] them and the music is just [OOV] [OOV] that plays over and over again in almost every scene there is [OOV] music [OOV] and [OOV] [OOV] away [OOV] and the [OOV] still doesn't close for [OOV] all [OOV] [OOV] this is a truly bad film [OOV] only [OOV] is to look back on the [OOV] that was the [OOV] and have a good old laugh at how bad everything was back then\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Decode the first sequence in the dataset\n",
    "decoded_sequence = \" \".join(inverted_word_index[i] for i in x_train[0])\n",
    "decoded_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"big big bad music and a these are the to best this terrible movie i love horror movies and i've seen but this had got to be on of the worst ever made the plot is and the acting is an the script is completely the best is the end with the and how he out who the killer is it's just so written the are and funny in the is big of men those that show off their that men actually them and the music is just that plays over and over again in almost every scene there is music and away and the still doesn't close for all this is a truly bad film only is to look back on the that was the and have a good old laugh at how bad everything was back then\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_to_remove = [word_index['br']+index_from, oov_char, start_char]\n",
    "\n",
    "x_train = [[w for w in x_train[i] if w not in words_to_remove] for i in range(len(x_train))]\n",
    "decoded_sequence = \" \".join([inverted_word_index[i] for i in x_train[0]])\n",
    "decoded_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Words we want to analyze\n",
    "\n",
    "w1 = 'good'\n",
    "w2 = 'bad'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 49, 75)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V = num_words\n",
    "assert max(word_index[w1], word_index[w2]) < V\n",
    "V, word_index[w1], word_index[w2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now get the dictionary of words to indexes and indexes to words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating 500 words\n"
     ]
    }
   ],
   "source": [
    "words = [w for w in word_index if word_index[w] < V]\n",
    "print(f'generating {V} words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build the co-occurrence matrix. Our implementation will not be the most efficient one, but it will serve the purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparsity is 0.085212\n"
     ]
    }
   ],
   "source": [
    "# Initialize co-occurrence matrix\n",
    "X = np.zeros((V, V))\n",
    "for s in x_train:\n",
    "    for i in range(1, len(s)):\n",
    "        j_indexes = i - np.arange(1, window+1)\n",
    "        j_indexes = j_indexes[j_indexes >= 0]\n",
    "        for j in j_indexes:\n",
    "            inc = 1.0 / (i - j)\n",
    "            X[s[i], s[j]] += inc\n",
    "            X[s[j], s[i]] += inc\n",
    "print(f'sparsity is {np.mean(X.flatten() == 0)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29396.116666667273, 228697, 250000)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Xp = X.flatten()\n",
    "Xp = Xp[Xp > 0]\n",
    "plt.hist(np.log(Xp))\n",
    "np.max(Xp), len(Xp), V*V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(228697,\n",
       " [(4, 4),\n",
       "  (4, 5),\n",
       "  (4, 6),\n",
       "  (4, 7),\n",
       "  (4, 8),\n",
       "  (4, 9),\n",
       "  (4, 11),\n",
       "  (4, 12),\n",
       "  (4, 13),\n",
       "  (4, 14)])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xmax = 1000\n",
    "eps=1e-3\n",
    "lr = 0.1\n",
    "beta = 0.99\n",
    "epochs = 200\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "w = 2 * (np.random.rand(2*V, emb_size) - 0.5) / (emb_size + 1)\n",
    "b = 2 * (np.random.rand(2*V) - 0.5) / (emb_size + 1)\n",
    "g_w_s = np.ones((2 * V, emb_size), dtype=np.float32)\n",
    "g_b_s = np.ones(2 * V, dtype=np.float32)\n",
    " \n",
    "indexes = []\n",
    "all_idx = np.arange(V)\n",
    "for i in range(V):\n",
    "    mask = X[i] != 0\n",
    "    if np.sum(mask) == 0: continue\n",
    "    for j in all_idx[mask]:\n",
    "        indexes.append((i, j))\n",
    "len(indexes), indexes[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 29336.49 0.9353\n",
      "1 6209.21 0.6538\n",
      "2 4902.44 0.6158\n",
      "3 3988.35 0.6617\n",
      "4 3610.17 0.6096\n",
      "5 3362.6 0.6588\n",
      "6 3271.74 0.6451\n",
      "7 3260.83 0.6787\n",
      "8 3127.32 0.6927\n",
      "9 3138.6 0.6291\n",
      "10 3102.58 0.6103\n",
      "11 3010.74 0.5691\n",
      "12 3014.3 0.5153\n",
      "13 2947.73 0.4247\n",
      "14 2921.75 0.6921\n",
      "15 2884.12 0.5402\n",
      "16 2934.4 0.4878\n",
      "17 2952.51 0.4632\n",
      "18 2919.06 0.5452\n",
      "19 2884.94 0.3393\n",
      "20 2894.71 0.1499\n",
      "21 2837.11 0.3681\n",
      "22 2870.28 0.3424\n",
      "23 2892.81 0.459\n",
      "24 2898.75 0.2965\n",
      "25 2853.2 0.2828\n",
      "26 2898.21 0.5094\n",
      "27 2895.6 0.5565\n",
      "28 2932.34 0.3574\n",
      "29 2830.02 0.2902\n",
      "30 2981.9 0.366\n",
      "31 2905.51 0.3182\n",
      "32 2858.98 0.2384\n",
      "33 2863.66 0.3331\n",
      "34 2883.8 0.313\n",
      "35 2850.05 0.2705\n",
      "36 2884.22 0.3442\n",
      "37 2881.82 0.332\n",
      "38 2855.93 0.5242\n",
      "39 2871.36 0.3849\n",
      "40 2859.14 0.2462\n",
      "41 2912.04 0.0688\n",
      "42 2854.57 0.1595\n",
      "43 2857.09 0.1715\n",
      "44 2891.69 0.3382\n",
      "45 2896.07 0.2782\n",
      "46 2847.04 0.2945\n",
      "47 2907.14 0.3263\n",
      "48 2905.86 0.1751\n",
      "49 2894.34 0.1864\n",
      "50 2883.74 0.2476\n",
      "51 2825.19 0.1768\n",
      "52 2896.64 0.1802\n",
      "53 2861.99 0.2818\n",
      "54 2866.78 0.3351\n",
      "55 2859.62 0.0729\n",
      "56 2858.5 0.124\n",
      "57 2899.0 0.4214\n",
      "58 2846.22 0.3261\n",
      "59 2822.6 0.2159\n",
      "60 2880.26 0.2957\n",
      "61 2855.42 0.1176\n",
      "62 2802.51 0.3336\n",
      "63 2847.1 0.323\n",
      "64 2882.44 0.2968\n",
      "65 2870.35 0.1659\n",
      "66 2835.92 0.2803\n",
      "67 2851.37 0.4731\n",
      "68 2825.15 0.4033\n",
      "69 2854.93 0.399\n",
      "70 2994.3 0.1703\n",
      "71 2839.81 0.2328\n",
      "72 2863.6 0.244\n",
      "73 2828.28 0.3533\n",
      "74 2864.24 0.3182\n",
      "75 2812.9 0.2654\n",
      "76 2870.89 0.2362\n",
      "77 2924.23 0.2014\n",
      "78 2841.78 0.1459\n",
      "79 2872.62 0.1883\n",
      "80 2846.18 0.2978\n",
      "81 2854.61 0.0856\n",
      "82 2928.48 0.3714\n",
      "83 2918.65 0.3613\n",
      "84 2843.9 0.4047\n",
      "85 2856.87 0.3816\n",
      "86 2888.37 0.3404\n",
      "87 2828.09 0.227\n",
      "88 2825.15 0.3258\n",
      "89 2879.74 0.4647\n",
      "90 2853.62 0.4378\n",
      "91 2860.07 0.4667\n",
      "92 2868.8 0.382\n",
      "93 2874.15 0.4446\n",
      "94 2897.63 0.403\n",
      "95 2926.88 0.2509\n",
      "96 2859.59 0.2541\n",
      "97 2847.25 0.2278\n",
      "98 2856.17 0.2657\n",
      "99 2877.02 0.4441\n",
      "100 2859.8 0.3263\n",
      "101 2836.69 0.4311\n",
      "102 2868.81 0.3295\n",
      "103 2866.72 0.2344\n",
      "104 2862.31 0.2429\n",
      "105 2877.69 0.3584\n",
      "106 2833.05 0.3634\n",
      "107 2840.23 0.3169\n",
      "108 2853.13 0.289\n",
      "109 2845.33 0.3713\n",
      "110 2840.97 0.0802\n",
      "111 2825.63 0.2804\n",
      "112 2873.39 0.2953\n",
      "113 2890.52 0.3683\n",
      "114 2862.66 0.2158\n",
      "115 2828.62 0.3113\n",
      "116 2887.35 0.2569\n",
      "117 2912.78 0.2991\n",
      "118 2865.17 0.283\n",
      "119 2859.76 0.3002\n",
      "120 2909.98 0.2842\n",
      "121 2869.42 0.3177\n",
      "122 2860.66 0.2738\n",
      "123 2838.16 0.3037\n",
      "124 2865.75 0.2129\n",
      "125 2857.23 0.3855\n",
      "126 2831.62 0.2347\n",
      "127 2852.77 0.1565\n",
      "128 2879.45 0.1913\n",
      "129 2895.03 0.2525\n",
      "130 2883.82 0.2754\n",
      "131 2834.62 0.1634\n",
      "132 2877.98 0.2072\n",
      "133 2845.63 0.2512\n",
      "134 2854.26 0.2661\n",
      "135 2865.83 0.1814\n",
      "136 2873.16 0.2726\n",
      "137 2865.38 0.0965\n",
      "138 2846.09 0.3436\n",
      "139 2843.44 0.3574\n",
      "140 2835.81 0.3783\n",
      "141 2832.5 0.1679\n",
      "142 2880.75 0.2923\n",
      "143 2865.58 0.1631\n",
      "144 2881.07 0.318\n",
      "145 2841.23 0.2159\n",
      "146 2850.65 0.3309\n",
      "147 2920.77 0.3582\n",
      "148 2886.46 0.2826\n",
      "149 2846.56 0.1522\n",
      "150 2857.49 0.1597\n",
      "151 2854.08 0.306\n",
      "152 2898.73 0.1945\n",
      "153 2892.94 0.3461\n",
      "154 2892.73 0.0649\n",
      "155 2863.29 0.1872\n",
      "156 2925.35 0.2862\n",
      "157 2866.38 0.392\n",
      "158 2852.14 0.3177\n",
      "159 2862.53 0.2427\n",
      "160 2887.05 0.1101\n",
      "161 2897.83 0.2477\n",
      "162 2845.42 0.3492\n",
      "163 2866.64 0.3753\n",
      "164 2859.69 0.2307\n",
      "165 2887.08 0.3623\n",
      "166 2892.95 0.453\n",
      "167 2863.77 0.3296\n",
      "168 2871.84 0.4952\n",
      "169 2872.49 0.1806\n",
      "170 2920.95 0.3051\n",
      "171 2852.99 0.2731\n",
      "172 2862.04 0.2717\n",
      "173 2860.71 0.1587\n",
      "174 2856.53 0.3667\n",
      "175 2887.91 0.1756\n",
      "176 2856.9 0.2811\n",
      "177 2862.93 0.1948\n",
      "178 2858.65 0.2223\n",
      "179 2860.74 0.1597\n",
      "180 2896.03 0.3692\n",
      "181 2836.96 0.2995\n",
      "182 2840.11 0.4555\n",
      "183 2848.03 0.3927\n",
      "184 2845.35 0.373\n",
      "185 2845.64 0.3636\n",
      "186 2894.47 0.3123\n",
      "187 2893.3 0.182\n",
      "188 2906.1 0.2608\n",
      "189 2844.49 0.3776\n",
      "190 2846.85 0.3208\n",
      "191 2890.3 0.2\n",
      "192 2849.95 0.1398\n",
      "193 2835.89 0.4797\n",
      "194 2921.5 0.2738\n",
      "195 2843.03 0.3544\n",
      "196 2817.66 0.3979\n",
      "197 2858.34 0.3766\n",
      "198 2848.66 0.4891\n",
      "199 2859.02 0.1241\n"
     ]
    }
   ],
   "source": [
    "def J():\n",
    "    result = 0\n",
    "    for i in range(V):\n",
    "        for j in range(V):\n",
    "            result += f(X[i][j]) * np.power(\n",
    "                np.dot(w[i], w[j+V]) + b[i] + b[j+V] - np.log1p(X[i][j]), 2)\n",
    "            \n",
    "    return result\n",
    "\n",
    "def f(x, alpha=0.75):\n",
    "    if x < Xmax:\n",
    "        return np.power(x / Xmax, alpha)\n",
    "    else:\n",
    "        return 1.0\n",
    "    \n",
    "def W(word):\n",
    "    ww = w[word_index[word]]\n",
    "    return ww / np.linalg.norm(ww)\n",
    "\n",
    "all_js = []\n",
    "for e in range(epochs):\n",
    "    cost = 0\n",
    "    shuffle(indexes)\n",
    "    for i, jj in indexes:\n",
    "        j = jj + V\n",
    "        weight = f(X[i][jj])\n",
    "        inner = (np.dot(w[i], w[j]) + b[i] + b[j] - np.log(X[i][jj]))\n",
    "        dwi = w[j] * weight * inner\n",
    "        dwj = w[i] * weight * inner\n",
    "        dbi = dbj = weight * inner\n",
    "        cost += weight * inner ** 2\n",
    "        w[i] -= np.clip(lr * dwi / np.sqrt(g_w_s[i] + eps), -1, 1)\n",
    "        w[j] -= np.clip(lr * dwj / np.sqrt(g_w_s[j] + eps), -1, 1)\n",
    "        b[i] -= np.clip(lr * dbi / np.sqrt(g_b_s[i] + eps), -1, 1)\n",
    "        b[j] -= np.clip(lr * dbj / np.sqrt(g_b_s[j] + eps), -1, 1)\n",
    "\n",
    "        g_w_s[i] = beta * g_w_s[i] + np.square(dwi)\n",
    "        g_w_s[j] = beta * g_w_s[j] + np.square(dwj)\n",
    "        g_b_s[i] = beta * g_b_s[i] + np.square(dbi)\n",
    "        g_b_s[j] = beta * g_b_s[j] + np.square(dbj)\n",
    "            \n",
    "    all_js.append(cost)\n",
    "    \n",
    "    print(e, np.round(cost, 2), np.round(W('good').dot(W('bad')), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now explain with your own words how Glove works.  Find which word is a synonym for `positive` and an antonym for `positive`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GloVe works by creatubg a global co-occurrence matrix and using a log-bilinear regression model to get relationships between words. The model uses both global statistical information and local context, creating word vectors that are great in capturing semantic and syntactic patterns in text.\n",
    "\n",
    "Good is the synonym for positive, and bad is the antonym for positive. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Convolution Based NLP\n",
    "\n",
    "In an attempt to create our first generator network before we start using transformers, you will build a large language model using a Convolutional (causal) and Embeddings.\n",
    "\n",
    "We will split this task into two task.\n",
    "\n",
    "- First task is to try to predict the next word.\n",
    "- Second task is to implement and train a network that will use the head to predict the sentiment of the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import keras\n",
    "from keras import preprocessing\n",
    "from keras.datasets import imdb\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[79, 76, 83, 83, 86, 7, 94, 86, 89, 83, 75]\n",
      "['[START]', '[OOV]', '[OOV]', 'h', 'e', 'l', 'l', 'o', ' ', 'w', 'o', 'r', 'l', 'd']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self):\n",
    "        self.chars = ['\\00', '\\01'] + sorted(list(set(string.printable)))\n",
    "        self.vocab_size = len(self.chars)\n",
    "        self.stoi = { ch:i for i,ch in enumerate(self.chars) }\n",
    "        self.itos = { i:ch for i,ch in enumerate(self.chars) }\n",
    "        \n",
    "        self.itos[0] = '[START]'\n",
    "        self.itos[1] = '[OOV]'\n",
    "                            \n",
    "    def encode(self, sentence):\n",
    "        return [self.stoi[c] if c in self.stoi else 1 for c in sentence]\n",
    "    \n",
    "    def decode(self, indexes):\n",
    "        return [self.itos[i] for i in indexes]\n",
    "    \n",
    "    def start(self): return '\\00'\n",
    "\n",
    "    def oov(self): return '\\01'\n",
    "    \n",
    "tokenizer = Tokenizer()\n",
    "print(tokenizer.encode('hello world'))\n",
    "print(tokenizer.decode([0] + [1] + tokenizer.encode('\\x96hello world')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 1024\n",
    "emb_size = 50\n",
    "\n",
    "start_char = 1\n",
    "oov_char = 2\n",
    "index_from = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(\n",
    "    start_char=start_char, oov_char=oov_char, index_from=index_from\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = imdb.get_word_index()\n",
    "inverted_word_index = dict(\n",
    "    (i + index_from, word) for (word, i) in word_index.items()\n",
    ")\n",
    "# Update `inverted_word_index` to include `start_char` and `oov_char`\n",
    "inverted_word_index[start_char] = tokenizer.start()\n",
    "inverted_word_index[oov_char] = tokenizer.oov()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  7,  7, ..., 72, 83, 83])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def retokenize(text):\n",
    "    return [tokenizer.encode('  '.join([inverted_word_index[idx] for idx in text[i]])) \n",
    "            for i in range(len(text))]\n",
    "\n",
    "x_train = retokenize(x_train)\n",
    "x_test = retokenize(x_test)\n",
    "np.array(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((38024315,), (36523678,))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_c = np.concatenate(x_train)\n",
    "x_test_c = np.concatenate(x_test)\n",
    "x_train_c.shape, x_test_c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalConv1d(nn.Module):\n",
    "    def __init__(self, embedding_size, window, dilation):\n",
    "        super().__init__()\n",
    "        self.conv1d = nn.Conv1d(\n",
    "            embedding_size, embedding_size, \n",
    "            kernel_size=window, groups=embedding_size,\n",
    "            padding=(window-1)*dilation, dilation=dilation)\n",
    "        self.ln = nn.LayerNorm(embedding_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # pytorch requires C to be in dimension 1\n",
    "        x = x.permute(0, 2, 1) # (B, C, T)\n",
    "        # we do convolution from C -> C, but with only one group\n",
    "        # that means that we are doing a depthwise convolution\n",
    "        # so that we use the same filter for each embedding\n",
    "        x = self.conv1d(x)\n",
    "        # after the convolution, we need to restore the dimension\n",
    "        # and remove the extra right padding\n",
    "        x = x[:, :, :-self.conv1d.padding[0]]\n",
    "        x = x.permute(0, 2, 1) # (B, T, C)\n",
    "        return self.ln(x)\n",
    "    \n",
    "class Head(nn.Module):\n",
    "    def __init__(self, embedding_size, vocab_size, max_len, window, dropout):\n",
    "        super().__init__()\n",
    "        self.max_len = max_len\n",
    "        self.wte = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.wpe = nn.Embedding(max_len, embedding_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.conv1d_0 = CausalConv1d(embedding_size, window, dilation=1)\n",
    "        self.conv1d_1 = CausalConv1d(embedding_size, window, dilation=1)\n",
    "        self.lin_0 = nn.Linear(embedding_size, embedding_size)\n",
    "        self.lin_1 = nn.Linear(embedding_size, embedding_size)\n",
    "        self.lin_2 = nn.Linear(embedding_size, embedding_size)\n",
    "        self.ln_0 = nn.LayerNorm(embedding_size)\n",
    "        self.time_shift = nn.ZeroPad2d((0,0,1,0)) # TRICK: time-mix\n",
    "        \n",
    "    def forward(self, idx):\n",
    "        device = idx.device\n",
    "        B, T = idx.shape\n",
    "        \n",
    "        assert T <= self.max_len\n",
    "\n",
    "        tok_emb = self.wte(idx) # (B, T, C)\n",
    "        \n",
    "        B, T, C = tok_emb.shape\n",
    "        \n",
    "        pos = torch.arange(\n",
    "            0, T, dtype=torch.long, device=device).unsqueeze(0)\n",
    "        pos_emb = self.wpe(pos)\n",
    "        x = self.dropout(tok_emb + pos_emb)\n",
    "        x = self.ln_0(x)\n",
    "        x = torch.cat([self.time_shift(x)[:,:T,:C//2], x[:,:T,C//2:]], dim=2) # TRICK: time-mix\n",
    "        # idea is to do Linear C dimension\n",
    "        # followed by a causal filter on the T dimension\n",
    "        x = nn.GELU(approximate='tanh')(self.lin_0(x))\n",
    "        x = nn.GELU(approximate='tanh')(self.conv1d_0(x))\n",
    "\n",
    "        x = nn.GELU(approximate='tanh')(self.lin_1(x))\n",
    "        x = nn.GELU(approximate='tanh')(self.conv1d_1(x))\n",
    "\n",
    "        x = nn.GELU(approximate='tanh')(self.lin_2(x))\n",
    "        return x\n",
    "\n",
    "class LLM(nn.Module):\n",
    "    def __init__(self, embedding_size, vocab_size):\n",
    "        super().__init__()\n",
    "        self.ln = nn.LayerNorm(embedding_size)\n",
    "        self.linv = nn.Linear(embedding_size, vocab_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.linv(self.ln(x))\n",
    "            \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wte.weight torch.Size([102, 50])\n",
      "wpe.weight torch.Size([1024, 50])\n",
      "conv1d_0.conv1d.weight torch.Size([50, 1, 1024])\n",
      "conv1d_0.conv1d.bias torch.Size([50])\n",
      "conv1d_0.ln.weight torch.Size([50])\n",
      "conv1d_0.ln.bias torch.Size([50])\n",
      "conv1d_1.conv1d.weight torch.Size([50, 1, 1024])\n",
      "conv1d_1.conv1d.bias torch.Size([50])\n",
      "conv1d_1.ln.weight torch.Size([50])\n",
      "conv1d_1.ln.bias torch.Size([50])\n",
      "lin_0.weight torch.Size([50, 50])\n",
      "lin_0.bias torch.Size([50])\n",
      "lin_1.weight torch.Size([50, 50])\n",
      "lin_1.bias torch.Size([50])\n",
      "lin_2.weight torch.Size([50, 50])\n",
      "lin_2.bias torch.Size([50])\n",
      "ln_0.weight torch.Size([50])\n",
      "ln_0.bias torch.Size([50])\n",
      "\n",
      "ln.weight torch.Size([50])\n",
      "ln.bias torch.Size([50])\n",
      "linv.weight torch.Size([102, 50])\n",
      "linv.bias torch.Size([102])\n"
     ]
    }
   ],
   "source": [
    "head = Head(emb_size, vocab_size, maxlen, maxlen, 0.15)\n",
    "head = head.to(device)\n",
    "model = LLM(emb_size, vocab_size)\n",
    "model = model.to(device)\n",
    "for name, params in head.named_parameters():\n",
    "    print(name, params.shape)\n",
    "print()\n",
    "for name, params in model.named_parameters():\n",
    "    print(name, params.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "loss_f = F.cross_entropy\n",
    "\n",
    "def get_batch(data):\n",
    "    ix = torch.randint(len(data) - maxlen, (batch_size,))\n",
    "    x = torch.stack([\n",
    "        torch.from_numpy(data[i:i+maxlen].astype(np.int64)) for i in ix])\n",
    "    y = torch.stack([\n",
    "        torch.from_numpy(data[i+1:i+maxlen+1].astype(np.int64)) for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(x_train_c, x_test_c, eval_iters=100):\n",
    "    def _internal(model):\n",
    "        dataset = {'train': x_train_c, 'val': x_test_c}\n",
    "        out = {}\n",
    "        model.eval()\n",
    "        for split in ['train', 'val']:\n",
    "            losses = torch.zeros(eval_iters)\n",
    "            for k in range(eval_iters):\n",
    "                X, Y = get_batch(dataset[split])\n",
    "                p = model(head(X))\n",
    "                B, T, C = p.shape\n",
    "                loss = loss_f(p.view(B*T, C), Y.view(B*T))\n",
    "                losses[k] = loss.item()\n",
    "            out[split] = losses.mean()\n",
    "        model.train()\n",
    "        return out\n",
    "    return _internal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 7, 91, 79,  ...,  7, 94, 72], device='cuda:0'),\n",
       " tensor([91, 79, 76,  ..., 94, 72, 90], device='cuda:0'))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1, y1 = get_batch(x_train_c)\n",
    "x1[0], y1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def get_lr_func(warmup_iters, learning_rate, lr_decay_iters,  min_lr):\n",
    "    def __get_lr__(it):\n",
    "        nonlocal decay_ratio\n",
    "        # 1) linear warmup for warmup_iters steps\n",
    "        if it < warmup_iters:\n",
    "            return learning_rate * it / warmup_iters\n",
    "        # 2) if it > lr_decay_iters, return min learning rate\n",
    "        if it > lr_decay_iters:\n",
    "            return min_lr\n",
    "        # 3) in between, use cosine decay down to min learning rate\n",
    "        decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
    "        assert 0 <= decay_ratio <= 1\n",
    "        coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n",
    "        return min_lr + coeff * (learning_rate - min_lr)\n",
    "\n",
    "    decay_ratio = 0\n",
    "    return __get_lr__\n",
    "\n",
    "learning_rate = 0.005\n",
    "get_lr = get_lr_func(1000, learning_rate, 100000, learning_rate/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_num = 0\n",
    "logs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\anaconda3\\envs\\tf\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.8904, val loss 4.8912\n",
      "step 1000: train loss 1.7008, val loss 1.6850\n",
      "step 2000: train loss 1.5265, val loss 1.5129\n",
      "step 3000: train loss 1.4746, val loss 1.4551\n",
      "step 4000: train loss 1.4517, val loss 1.4335\n",
      "step 5000: train loss 1.4416, val loss 1.4197\n",
      "step 6000: train loss 1.4268, val loss 1.4059\n",
      "step 7000: train loss 1.4222, val loss 1.4031\n",
      "step 8000: train loss 1.4147, val loss 1.3954\n",
      "step 9000: train loss 1.4100, val loss 1.3925\n",
      "step 10000: train loss 1.4114, val loss 1.3886\n",
      "step 11000: train loss 1.4042, val loss 1.3836\n",
      "step 12000: train loss 1.4020, val loss 1.3853\n",
      "step 13000: train loss 1.4016, val loss 1.3821\n",
      "step 14000: train loss 1.3981, val loss 1.3846\n",
      "step 15000: train loss 1.3980, val loss 1.3780\n",
      "step 16000: train loss 1.3961, val loss 1.3777\n",
      "step 17000: train loss 1.3969, val loss 1.3732\n",
      "step 18000: train loss 1.3940, val loss 1.3768\n",
      "step 19000: train loss 1.3922, val loss 1.3726\n",
      "step 20000: train loss 1.3895, val loss 1.3746\n",
      "step 21000: train loss 1.3903, val loss 1.3702\n",
      "step 22000: train loss 1.3897, val loss 1.3680\n",
      "step 23000: train loss 1.3880, val loss 1.3684\n",
      "step 24000: train loss 1.3879, val loss 1.3668\n",
      "step 25000: train loss 1.3866, val loss 1.3623\n",
      "step 26000: train loss 1.3840, val loss 1.3648\n",
      "step 27000: train loss 1.3846, val loss 1.3645\n",
      "step 28000: train loss 1.3831, val loss 1.3608\n",
      "step 29000: train loss 1.3850, val loss 1.3639\n",
      "step 30000: train loss 1.3797, val loss 1.3617\n",
      "step 31000: train loss 1.3791, val loss 1.3630\n",
      "step 32000: train loss 1.3804, val loss 1.3585\n",
      "step 33000: train loss 1.3792, val loss 1.3596\n",
      "step 34000: train loss 1.3774, val loss 1.3595\n",
      "step 35000: train loss 1.3780, val loss 1.3580\n",
      "step 36000: train loss 1.3736, val loss 1.3581\n",
      "step 37000: train loss 1.3763, val loss 1.3586\n",
      "step 38000: train loss 1.3765, val loss 1.3549\n",
      "step 39000: train loss 1.3760, val loss 1.3548\n",
      "step 40000: train loss 1.3732, val loss 1.3561\n",
      "step 41000: train loss 1.3724, val loss 1.3550\n",
      "step 42000: train loss 1.3731, val loss 1.3523\n",
      "step 43000: train loss 1.3742, val loss 1.3506\n",
      "step 44000: train loss 1.3728, val loss 1.3534\n",
      "step 45000: train loss 1.3726, val loss 1.3536\n",
      "step 46000: train loss 1.3729, val loss 1.3539\n",
      "step 47000: train loss 1.3719, val loss 1.3465\n",
      "step 48000: train loss 1.3680, val loss 1.3488\n",
      "step 49000: train loss 1.3682, val loss 1.3510\n"
     ]
    }
   ],
   "source": [
    "epochs = 50000\n",
    "\n",
    "params = list(head.parameters()) + list(model.parameters())\n",
    "estimate_loss_f = estimate_loss(x_train_c, x_test_c)\n",
    "optimizer = torch.optim.AdamW(params, lr=learning_rate)\n",
    "\n",
    "for iter_num in range(iter_num, iter_num + epochs):\n",
    "    learning_rate = get_lr(iter_num)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = learning_rate\n",
    "    if iter_num % 1000 == 0:\n",
    "        losses = estimate_loss_f(model)\n",
    "        print(\n",
    "            f'step {iter_num}: train loss {losses[\"train\"]:.4f}, '\n",
    "            f'val loss {losses[\"val\"]:.4f}')\n",
    "        logs.append((iter_num, losses['train'], losses['val']))\n",
    "    \n",
    "    xb, yb = get_batch(x_train_c)\n",
    "    pb = model(head(xb))\n",
    "    B, T, C = pb.shape\n",
    "    loss = loss_f(pb.view(B*T, C), yb.view(B*T))\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#plt.plot([v[1] for v in logs][3:], label='train')\n",
    "#plt.plot([v[2] for v in logs][3:], label='val')\n",
    "#plt.legend()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  7,  7, 87, 83, 76, 72, 90, 76,  7,  7, 78, 80, 93, 76,  7,  7,\n",
       "       91, 79, 80])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_c[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['[START]', ' ', ' ', 'p', 'l', 'e', 'a', 's', 'e', ' ', ' ', 'g',\n",
       "       'i', 'v', 'e', ' ', ' ', 't', 'h', 'i'], dtype='<U7')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(tokenizer.decode(x_test_c[0:20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(idx, max_new_tokens):\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -maxlen:]\n",
    "        logits = model(head(idx_cond))\n",
    "        logits = logits[:, 0, :]\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        idx_next = torch.multinomial(probs, num_samples=1)\n",
    "        if idx_next == 0: break\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  7,  7, 87, 83, 76, 72, 90, 76,  7,  7, 78, 80, 93, 76,  7,\n",
       "         7, 91, 79, 80,  7, 91,  7,  7,  7, 79, 76, 85, 79, 80, 76, 76,\n",
       "        96, 79, 79,  7, 91, 76, 89, 91, 80, 96, 79, 80, 86, 80, 76, 86,\n",
       "        96, 80, 82,  7, 79, 74,  7,  7,  7,  7, 76, 89, 76, 90, 76, 91,\n",
       "        72, 76,  7,  7, 76, 77, 90,  7,  7, 75, 80, 83, 74,  7, 76,  7,\n",
       "        72,  7, 77, 14,  7, 76, 76, 80,  7, 76, 79,  7,  7, 80, 76, 76,\n",
       "        85,  7, 89, 92,  7, 73,  7, 75, 90,  7, 79, 76, 91, 86, 76, 96,\n",
       "        72, 96, 76, 86, 92, 94, 86, 89]], dtype=int64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = np.array([x_test_c[0:20]]).astype(np.int64)\n",
    "result = generate(torch.from_numpy(idx).to(device), max_new_tokens=100).detach().cpu().numpy()\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[START]  please  give  thi t   henhieeyhh tertiyhioieoyik hc    eresetae  efs  dilc e a f' eei eh  ieen ru b ds hetoeyayeouwor\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_sequence = ''.join(tokenizer.decode(result[0]))\n",
    "decoded_sequence"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
