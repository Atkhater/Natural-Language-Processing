{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d84d6c8",
   "metadata": {},
   "source": [
    "Homework can be done in pairs, but each person needs to submit a the python book.\n",
    "\n",
    "\n",
    "# 1.\tIn the last layer of a DL network or logistic regression with multiple outputs, what’s the difference between using a softmax activation and a sigmoid?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be68d683-9fbf-43ce-a06e-20d2b6731497",
   "metadata": {},
   "source": [
    "Softmax is used for multi-class classifaction. The softmax will return the output with a probability, where the sum of ALL outputs will add up to 1.\n",
    "Sigmoid is used for multi-label classification. The sigmoid will return the output with a probability, but without considering other outputs. Does not have to add up to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daec8765",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2.\tWhat’s the advantage of using RELU vs sigmoid for intermediate layers? What’s the disadvantage?\n",
    "Relu tackles the vanishing gradient problem since its gradient is 0 or 1. \n",
    "Sigmoid can have the vanishing gradient problem as it can stall as it reachs values close to 0.\n",
    "\n",
    "Relu is much faster computationally, while sigmoid is more expensive due to the exponential function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed65765",
   "metadata": {},
   "source": [
    "# 3.\tSome people have suggested an all convolutional network instead of using Dense layers at the end. What’s the advantage of that?\n",
    "\n",
    "Fewer parameters to compute. Less chances of overfitting. Computationally efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ac0977",
   "metadata": {},
   "source": [
    "# 4.\tWhat’s L1 and L2 regularization?\n",
    "\n",
    "Loss functions\n",
    "L1 can perform feature selection. Good for only choose a few features.\n",
    "L2 uses the square of the weights, which can shrink the weights to 0 (as they are <1). prevents the model from changing drastically from small changes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d555b6f6-869e-4458-95b2-68966fb809d9",
   "metadata": {},
   "source": [
    "# 5. What's the difference of using MSE or crossentropy as a loss function? What's Hubber loss function?\n",
    "\n",
    "MSE used typically for regression. Can be sensitive to outliers between the predicted values and actual values.\n",
    "\n",
    "cross-entropy looks tat the probabilities between the true and predicted distrubution. Used for binary classifaction and multi classifcation.\n",
    "\n",
    "Huberloss combines MSE and Mean Absolute Error, making it less sensitive to outliers. It makes a comparison between the errors of MSE and a set threshold, choosing between MSE or MAE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2fd40d",
   "metadata": {},
   "source": [
    "# 6.\tReview the two papers of the module and summarize them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ee5881-5c08-416e-9c52-96772ec45b1f",
   "metadata": {},
   "source": [
    "**Convolutional Neural Networks for Sentence Classification**\n",
    "Yoon Kim\n",
    "\n",
    "Paper focuses on CNN with one layer convolution using word vectors utilizing word2vec. The model practically represents the sentences as a matrix, which is fed through a convolutional layer where each filter extracts one feature. Afterwards, its fed to a max-over-time pooling over the geature map to take the max value. Lastly, the model is dropped into a fully connected layter with dropout with a softmax output for regularization. They performed their model against benchmarks \n",
    "* (MR)Movie reviews\n",
    "* (SST-1 and SST-2)Stanford Sentiment Treebank\n",
    "* (Subj)Subjectivity dataset\n",
    "* (TREC) question dataset\n",
    "* (CR) Customer reviews\n",
    "* (MPQA) Opinion polarity detection\n",
    "\n",
    "Kim and the team utilized four different model variations  while utilizing word2vec. They were able to show that their CNN_rand model didn't perform as well, while their other models did, setting out to prove that a simple CNN with one layer of convolution performs well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e24d05-87e2-465a-9776-e2c337515ff3",
   "metadata": {},
   "source": [
    "**A Convolutional Neural Network for Modelling Sentences**\n",
    "Nal Kalchbrenner Edward Grefenstette Phil Blussom\n",
    "\n",
    "Paper introducess DCNN (Dynamic Convolutional Neural Network). It's utilized for semantic modelling of sentences. \n",
    "\n",
    "The model is as follows:\n",
    "* K-max pooling - which allows the sentences to be modeled for wide convolutional layers. Allows the pooling of the most prominent features that may be far apart.\n",
    "* Folding - which practically halves the number of the rows by summing up every two rows.\n",
    "* Wide convolution - utilizing one-dimensional convolution\n",
    "* Dynamic k-max pooling - where the value of k is determined by the length of the sentences and the depth of the network.\n",
    "\n",
    "They ran numerous expirements, where specifically on the Twitter sentiment data set, DCNN outperformed the other models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cf936e",
   "metadata": {},
   "source": [
    "For the next two problems, you will use the following code packages (you will implement the solution using SKLearn Logistic Regression and PyTorch. Present your results in a tabular format with 3 result columns: Naive Bayes, SKLearn Logic Regression, PyTorch Logistic Regression.\n",
    "\n",
    "Note: for multiclass, you will use logits (the output of the linear layer before softmax). Look at the documentation for ```torch.nn.CrossEntropyLoss()```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b2261062",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3aaf141c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: train loss = 0.6931, test loss = 0.6928\n",
      "Iteration 1: train loss = 0.6921, test loss = 0.6925\n",
      "Iteration 2: train loss = 0.6912, test loss = 0.6921\n",
      "Iteration 3: train loss = 0.6903, test loss = 0.6918\n",
      "Iteration 4: train loss = 0.6894, test loss = 0.6914\n",
      "Iteration 5: train loss = 0.6885, test loss = 0.6910\n",
      "Iteration 6: train loss = 0.6876, test loss = 0.6907\n",
      "Iteration 7: train loss = 0.6867, test loss = 0.6903\n",
      "Iteration 8: train loss = 0.6858, test loss = 0.6900\n",
      "Iteration 9: train loss = 0.6849, test loss = 0.6896\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Model, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "x_train_pt = torch.tensor(x_train_m[:1000],dtype=torch.float32)\n",
    "y_train_pt = torch.tensor(y_train[:1000],dtype=torch.float32).unsqueeze(1)\n",
    "x_test_pt = torch.tensor(x_test_m[:1000],dtype=torch.float32)\n",
    "y_test_pt = torch.tensor(y_test[:1000],dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "train_data = TensorDataset(x_train_pt, y_train_pt)\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "\n",
    "model = Model(x_train_m.shape[1])\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "max_iter = 10\n",
    "log_interval = 1\n",
    "\n",
    "for iter_num in range(max_iter):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        p_train = model(inputs)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss_train = loss_fn(p_train, labels)\n",
    "        \n",
    "        # backprop step\n",
    "        loss_train.backward()\n",
    "\n",
    "        # update weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss_train.item()\n",
    "    \n",
    "    # Log the training loss at intervals\n",
    "    if iter_num % log_interval == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            p_test = model(x_test_pt)\n",
    "            loss_test = loss_fn(p_test, y_test_pt).item()\n",
    "        \n",
    "        avg_epoch_loss = epoch_loss / len(train_loader)\n",
    "        print(f'Iteration {iter_num}: train loss = {avg_epoch_loss:.4f}, test loss = {loss_test:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0bc80a12-b991-4523-add7-d5638eb1b66a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9460\n",
      "Test Accuracy: 0.7250\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    train_outputs = model(x_train_pt)\n",
    "    test_outputs = model(x_test_pt)\n",
    "    \n",
    "    train_accuracy = ((train_outputs > 0) == y_train_pt).float().mean().item()\n",
    "    test_accuracy = ((test_outputs > 0) == y_test_pt).float().mean().item()\n",
    "\n",
    "print(f'Train Accuracy: {train_accuracy:.4f}')\n",
    "print(f'Test Accuracy: {test_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060155a4",
   "metadata": {},
   "source": [
    "## 6.1.\tUsing the same data from HW-4, exercise 3, create a Deep Learning classifier to perform sentiment analysis of the movie reviews. Perform the same comparison you did in HW-4 w.r.t. Naïve Bayes.  You will present the result in a table with accuracy for columns `Naïve Bayes`, `LogisticRegression` and `Deep Learning`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ecb2ef86-6a41-467c-bf11-3a9e352f5284",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[START] this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert redford's is an amazing actor and now the same being director norman's father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for retail and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also congratulations to the two little boy's that played the part's of norman and paul they were just brilliant children are often left out of the praising list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "start_char = 1\n",
    "oov_char = 2\n",
    "index_from = 3\n",
    "maxlen = 5000\n",
    "# Retrieve the training sequences.\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(\n",
    "    start_char=start_char, oov_char=oov_char, index_from=index_from, maxlen=maxlen\n",
    ")\n",
    "word_index = imdb.get_word_index()\n",
    "# Reverse the word index to obtain a dict mapping indices to words\n",
    "# And add `index_from` to indices to sync with `x_train`\n",
    "inverted_word_index = dict(\n",
    "    (i + index_from, word) for (word, i) in word_index.items()\n",
    ")\n",
    "# Update `inverted_word_index` to include `start_char` and `oov_char`\n",
    "inverted_word_index[0] = \"[PAD]\"\n",
    "inverted_word_index[index_from] = \"[NOP]\"\n",
    "inverted_word_index[start_char] = \"[START]\"\n",
    "inverted_word_index[oov_char] = \"[OOV]\"\n",
    "# Decode the first sequence in the dataset\n",
    "x_train_corpus = [\" \".join([inverted_word_index[j] for j in x_train[i]]) for i in range(len(x_train))]\n",
    "x_test_corpus = [\" \".join([inverted_word_index[j] for j in x_test[i]]) for i in range(len(x_test))]\n",
    "decoded_sequence = x_train_corpus[0]\n",
    "decoded_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a308743-6175-447c-b3a4-f2b0355261a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 88588)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = [inverted_word_index[i] for i in range(len(inverted_word_index))]\n",
    "pipe = Pipeline([('count', CountVectorizer(vocabulary=vocabulary, lowercase=False)), ('tfid', TfidfTransformer())]).fit(x_train_corpus)\n",
    "x_train_m = pipe.transform(x_train_corpus).toarray()\n",
    "x_train_m.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31c13964-99d8-4c48-b239-209df3ef3bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(max_iter=10000).fit(x_train_m, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f81042ee-2edd-4a53-8a49-51ef66551d83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.93304, 0.88312)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_m = pipe.transform(x_test_corpus).toarray()\n",
    "clf.score(x_train_m, y_train), clf.score(x_test_m, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51cdb9bd-7020-4ae9-8e59-bccbfec0c545",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([0.80302108, 0.19697892]), 0),\n",
       " (array([0.0341813, 0.9658187]), 1),\n",
       " (array([0.30628318, 0.69371682]), 1),\n",
       " (array([0.55254275, 0.44745725]), 0),\n",
       " (array([0.11605194, 0.88394806]), 1),\n",
       " (array([0.41074334, 0.58925666]), 1),\n",
       " (array([0.22513766, 0.77486234]), 1),\n",
       " (array([0.72476545, 0.27523455]), 0),\n",
       " (array([0.1522964, 0.8477036]), 0),\n",
       " (array([0.21737369, 0.78262631]), 1),\n",
       " (array([0.31806929, 0.68193071]), 1),\n",
       " (array([0.90207633, 0.09792367]), 0),\n",
       " (array([0.87058069, 0.12941931]), 0),\n",
       " (array([0.76029225, 0.23970775]), 0),\n",
       " (array([0.03887468, 0.96112532]), 1),\n",
       " (array([0.99114453, 0.00885547]), 0),\n",
       " (array([0.07177949, 0.92822051]), 1),\n",
       " (array([0.42042295, 0.57957705]), 0),\n",
       " (array([0.79430436, 0.20569564]), 0),\n",
       " (array([0.83691742, 0.16308258]), 0),\n",
       " (array([0.06087934, 0.93912066]), 1),\n",
       " (array([0.04972793, 0.95027207]), 1),\n",
       " (array([0.58740323, 0.41259677]), 1),\n",
       " (array([0.10588575, 0.89411425]), 1),\n",
       " (array([0.22399681, 0.77600319]), 1),\n",
       " (array([0.33913322, 0.66086678]), 1),\n",
       " (array([0.73305342, 0.26694658]), 0),\n",
       " (array([0.21344311, 0.78655689]), 1),\n",
       " (array([0.07689068, 0.92310932]), 1),\n",
       " (array([0.98131007, 0.01868993]), 0),\n",
       " (array([0.13938672, 0.86061328]), 1),\n",
       " (array([0.44630693, 0.55369307]), 1),\n",
       " (array([0.33004061, 0.66995939]), 0),\n",
       " (array([0.96780759, 0.03219241]), 0),\n",
       " (array([0.70879113, 0.29120887]), 0),\n",
       " (array([0.96891016, 0.03108984]), 0),\n",
       " (array([0.04838123, 0.95161877]), 1),\n",
       " (array([0.15684371, 0.84315629]), 1),\n",
       " (array([0.67315459, 0.32684541]), 0),\n",
       " (array([0.96149249, 0.03850751]), 0),\n",
       " (array([0.12499902, 0.87500098]), 1),\n",
       " (array([0.07301631, 0.92698369]), 1),\n",
       " (array([0.83621285, 0.16378715]), 0),\n",
       " (array([0.29243429, 0.70756571]), 1),\n",
       " (array([0.08709392, 0.91290608]), 1),\n",
       " (array([0.20384045, 0.79615955]), 1),\n",
       " (array([0.81589479, 0.18410521]), 0),\n",
       " (array([0.93848557, 0.06151443]), 0),\n",
       " (array([0.84773596, 0.15226404]), 0),\n",
       " (array([0.30088354, 0.69911646]), 1),\n",
       " (array([0.9242098, 0.0757902]), 0),\n",
       " (array([0.80014859, 0.19985141]), 0),\n",
       " (array([0.29233061, 0.70766939]), 1),\n",
       " (array([0.24070045, 0.75929955]), 1),\n",
       " (array([0.18179941, 0.81820059]), 1),\n",
       " (array([0.40645795, 0.59354205]), 1),\n",
       " (array([0.38540389, 0.61459611]), 1),\n",
       " (array([0.33181956, 0.66818044]), 1),\n",
       " (array([0.79892603, 0.20107397]), 0),\n",
       " (array([0.33363729, 0.66636271]), 0),\n",
       " (array([0.89633461, 0.10366539]), 0),\n",
       " (array([0.73751503, 0.26248497]), 0),\n",
       " (array([0.14522579, 0.85477421]), 1),\n",
       " (array([0.71327444, 0.28672556]), 0),\n",
       " (array([0.88870102, 0.11129898]), 0),\n",
       " (array([0.01904863, 0.98095137]), 1),\n",
       " (array([0.83246472, 0.16753528]), 0),\n",
       " (array([0.09488489, 0.90511511]), 1),\n",
       " (array([0.12110208, 0.87889792]), 1),\n",
       " (array([0.13696938, 0.86303062]), 1),\n",
       " (array([0.21786281, 0.78213719]), 1),\n",
       " (array([0.20887704, 0.79112296]), 1),\n",
       " (array([0.58134286, 0.41865714]), 1),\n",
       " (array([0.87150443, 0.12849557]), 0),\n",
       " (array([0.90549634, 0.09450366]), 0),\n",
       " (array([0.93283881, 0.06716119]), 0),\n",
       " (array([0.93103515, 0.06896485]), 0),\n",
       " (array([0.18708442, 0.81291558]), 1),\n",
       " (array([0.63241841, 0.36758159]), 0),\n",
       " (array([0.96581208, 0.03418792]), 0),\n",
       " (array([0.48579621, 0.51420379]), 1),\n",
       " (array([0.55334479, 0.44665521]), 0),\n",
       " (array([0.48081281, 0.51918719]), 1),\n",
       " (array([0.41932807, 0.58067193]), 1),\n",
       " (array([0.98228034, 0.01771966]), 0),\n",
       " (array([0.9943617, 0.0056383]), 0),\n",
       " (array([0.94009974, 0.05990026]), 0),\n",
       " (array([0.15680742, 0.84319258]), 1),\n",
       " (array([0.95445247, 0.04554753]), 0),\n",
       " (array([0.42631817, 0.57368183]), 1),\n",
       " (array([0.08996102, 0.91003898]), 1),\n",
       " (array([0.43722211, 0.56277789]), 1),\n",
       " (array([0.94721287, 0.05278713]), 0),\n",
       " (array([0.26552116, 0.73447884]), 1),\n",
       " (array([0.04141307, 0.95858693]), 1),\n",
       " (array([0.81385408, 0.18614592]), 0),\n",
       " (array([0.33876948, 0.66123052]), 1),\n",
       " (array([0.15957733, 0.84042267]), 1),\n",
       " (array([0.74382492, 0.25617508]), 0),\n",
       " (array([0.92494983, 0.07505017]), 0),\n",
       " (array([0.86002099, 0.13997901]), 1),\n",
       " (array([0.51841093, 0.48158907]), 1),\n",
       " (array([0.84288254, 0.15711746]), 0),\n",
       " (array([0.83544048, 0.16455952]), 0),\n",
       " (array([0.98166403, 0.01833597]), 0),\n",
       " (array([0.71654973, 0.28345027]), 0),\n",
       " (array([0.89240363, 0.10759637]), 0),\n",
       " (array([0.85798123, 0.14201877]), 0),\n",
       " (array([0.26873204, 0.73126796]), 1),\n",
       " (array([0.02249859, 0.97750141]), 1),\n",
       " (array([0.00679598, 0.99320402]), 1),\n",
       " (array([0.95315632, 0.04684368]), 0),\n",
       " (array([0.39546863, 0.60453137]), 0),\n",
       " (array([0.75490311, 0.24509689]), 0),\n",
       " (array([0.61023759, 0.38976241]), 0),\n",
       " (array([0.33562078, 0.66437922]), 0),\n",
       " (array([0.30449719, 0.69550281]), 1),\n",
       " (array([0.94337402, 0.05662598]), 0),\n",
       " (array([0.09985849, 0.90014151]), 1),\n",
       " (array([0.18751294, 0.81248706]), 1),\n",
       " (array([0.94106601, 0.05893399]), 0),\n",
       " (array([0.66972413, 0.33027587]), 0),\n",
       " (array([0.87615487, 0.12384513]), 0),\n",
       " (array([0.16716158, 0.83283842]), 1),\n",
       " (array([0.11506817, 0.88493183]), 1),\n",
       " (array([0.37202491, 0.62797509]), 1),\n",
       " (array([0.48159238, 0.51840762]), 1),\n",
       " (array([0.95110578, 0.04889422]), 0),\n",
       " (array([0.79669279, 0.20330721]), 0),\n",
       " (array([0.99610898, 0.00389102]), 0),\n",
       " (array([0.66281165, 0.33718835]), 1),\n",
       " (array([0.60744407, 0.39255593]), 0),\n",
       " (array([0.77384428, 0.22615572]), 0),\n",
       " (array([0.0516798, 0.9483202]), 1),\n",
       " (array([0.2743751, 0.7256249]), 1),\n",
       " (array([0.70649415, 0.29350585]), 0),\n",
       " (array([0.78995695, 0.21004305]), 0),\n",
       " (array([0.99055647, 0.00944353]), 0),\n",
       " (array([0.92750816, 0.07249184]), 0),\n",
       " (array([0.4583133, 0.5416867]), 0),\n",
       " (array([0.41864909, 0.58135091]), 1),\n",
       " (array([0.88129745, 0.11870255]), 0),\n",
       " (array([0.09842755, 0.90157245]), 1),\n",
       " (array([0.96679616, 0.03320384]), 0),\n",
       " (array([0.75965569, 0.24034431]), 0),\n",
       " (array([0.37686479, 0.62313521]), 1),\n",
       " (array([0.70394755, 0.29605245]), 1),\n",
       " (array([0.37490597, 0.62509403]), 0),\n",
       " (array([0.04075758, 0.95924242]), 1),\n",
       " (array([0.69468062, 0.30531938]), 0),\n",
       " (array([0.74398599, 0.25601401]), 0),\n",
       " (array([0.97064239, 0.02935761]), 0),\n",
       " (array([0.39188757, 0.60811243]), 0),\n",
       " (array([0.21675398, 0.78324602]), 1),\n",
       " (array([0.43902517, 0.56097483]), 0),\n",
       " (array([0.15995165, 0.84004835]), 1),\n",
       " (array([0.35691249, 0.64308751]), 0),\n",
       " (array([0.17959586, 0.82040414]), 1),\n",
       " (array([0.9142648, 0.0857352]), 0),\n",
       " (array([0.27870989, 0.72129011]), 1),\n",
       " (array([0.99028421, 0.00971579]), 0),\n",
       " (array([0.92231823, 0.07768177]), 0),\n",
       " (array([0.85692274, 0.14307726]), 0),\n",
       " (array([0.86319246, 0.13680754]), 0),\n",
       " (array([0.05017051, 0.94982949]), 1),\n",
       " (array([0.8842365, 0.1157635]), 0),\n",
       " (array([0.33656953, 0.66343047]), 0),\n",
       " (array([0.57222021, 0.42777979]), 1),\n",
       " (array([0.95018156, 0.04981844]), 0),\n",
       " (array([0.99655167, 0.00344833]), 0),\n",
       " (array([0.80461004, 0.19538996]), 0),\n",
       " (array([0.07136281, 0.92863719]), 1),\n",
       " (array([0.82276422, 0.17723578]), 0),\n",
       " (array([0.02544785, 0.97455215]), 1),\n",
       " (array([0.51179794, 0.48820206]), 1),\n",
       " (array([0.8674873, 0.1325127]), 0),\n",
       " (array([0.56218674, 0.43781326]), 1),\n",
       " (array([0.61335434, 0.38664566]), 0),\n",
       " (array([0.68840513, 0.31159487]), 0),\n",
       " (array([0.68897759, 0.31102241]), 0),\n",
       " (array([0.88394541, 0.11605459]), 0),\n",
       " (array([0.73106907, 0.26893093]), 0),\n",
       " (array([0.61459561, 0.38540439]), 0),\n",
       " (array([0.97990769, 0.02009231]), 0),\n",
       " (array([0.0551136, 0.9448864]), 1),\n",
       " (array([0.73130233, 0.26869767]), 0),\n",
       " (array([0.98580334, 0.01419666]), 0),\n",
       " (array([0.14862485, 0.85137515]), 1),\n",
       " (array([0.04342824, 0.95657176]), 1),\n",
       " (array([0.99107992, 0.00892008]), 0),\n",
       " (array([0.25904422, 0.74095578]), 1),\n",
       " (array([0.29591063, 0.70408937]), 1),\n",
       " (array([0.98245639, 0.01754361]), 0),\n",
       " (array([0.90230167, 0.09769833]), 0),\n",
       " (array([0.63132334, 0.36867666]), 0),\n",
       " (array([0.10531195, 0.89468805]), 1),\n",
       " (array([0.16783009, 0.83216991]), 1),\n",
       " (array([0.99346723, 0.00653277]), 0),\n",
       " (array([0.86286852, 0.13713148]), 0),\n",
       " (array([0.61382673, 0.38617327]), 0),\n",
       " (array([0.25139958, 0.74860042]), 1),\n",
       " (array([0.86174594, 0.13825406]), 0),\n",
       " (array([0.72463008, 0.27536992]), 0),\n",
       " (array([0.1911774, 0.8088226]), 1),\n",
       " (array([0.91724542, 0.08275458]), 0),\n",
       " (array([0.49561503, 0.50438497]), 1),\n",
       " (array([0.28942595, 0.71057405]), 1),\n",
       " (array([0.92603848, 0.07396152]), 0),\n",
       " (array([0.97398913, 0.02601087]), 0),\n",
       " (array([0.06430271, 0.93569729]), 1),\n",
       " (array([0.27567935, 0.72432065]), 1),\n",
       " (array([0.31528707, 0.68471293]), 1),\n",
       " (array([0.21014595, 0.78985405]), 1),\n",
       " (array([0.95900226, 0.04099774]), 0),\n",
       " (array([0.67204374, 0.32795626]), 0),\n",
       " (array([0.69241542, 0.30758458]), 0),\n",
       " (array([0.99086608, 0.00913392]), 0),\n",
       " (array([0.95562008, 0.04437992]), 0),\n",
       " (array([0.19162728, 0.80837272]), 1),\n",
       " (array([0.9329864, 0.0670136]), 0),\n",
       " (array([0.85164763, 0.14835237]), 0),\n",
       " (array([0.06311614, 0.93688386]), 1),\n",
       " (array([0.91708743, 0.08291257]), 0),\n",
       " (array([0.45420786, 0.54579214]), 1),\n",
       " (array([0.58678276, 0.41321724]), 1),\n",
       " (array([0.40529867, 0.59470133]), 1),\n",
       " (array([0.20921279, 0.79078721]), 1),\n",
       " (array([0.4580728, 0.5419272]), 1),\n",
       " (array([0.82838272, 0.17161728]), 0),\n",
       " (array([0.09026526, 0.90973474]), 1),\n",
       " (array([0.86977565, 0.13022435]), 0),\n",
       " (array([0.39252223, 0.60747777]), 1),\n",
       " (array([0.6751298, 0.3248702]), 0),\n",
       " (array([0.03814784, 0.96185216]), 1),\n",
       " (array([0.88711188, 0.11288812]), 0),\n",
       " (array([0.21067834, 0.78932166]), 1),\n",
       " (array([0.93954046, 0.06045954]), 0),\n",
       " (array([0.14011549, 0.85988451]), 1),\n",
       " (array([0.85372663, 0.14627337]), 0),\n",
       " (array([0.15626114, 0.84373886]), 1),\n",
       " (array([0.70837128, 0.29162872]), 0),\n",
       " (array([0.95043711, 0.04956289]), 0),\n",
       " (array([0.40433471, 0.59566529]), 1),\n",
       " (array([0.69057571, 0.30942429]), 0),\n",
       " (array([0.2129948, 0.7870052]), 1),\n",
       " (array([0.20073496, 0.79926504]), 1),\n",
       " (array([0.99588431, 0.00411569]), 0),\n",
       " (array([0.62330294, 0.37669706]), 0),\n",
       " (array([0.27086042, 0.72913958]), 1),\n",
       " (array([0.03124458, 0.96875542]), 1),\n",
       " (array([0.52832583, 0.47167417]), 1),\n",
       " (array([0.09578119, 0.90421881]), 1),\n",
       " (array([0.66278154, 0.33721846]), 0),\n",
       " (array([0.56145514, 0.43854486]), 1),\n",
       " (array([0.83541781, 0.16458219]), 0),\n",
       " (array([0.91767056, 0.08232944]), 0),\n",
       " (array([0.23922965, 0.76077035]), 0),\n",
       " (array([0.0636585, 0.9363415]), 1),\n",
       " (array([0.10788703, 0.89211297]), 1),\n",
       " (array([0.19208672, 0.80791328]), 1),\n",
       " (array([0.62666677, 0.37333323]), 0),\n",
       " (array([0.7526746, 0.2473254]), 0),\n",
       " (array([0.90457717, 0.09542283]), 0),\n",
       " (array([0.38620225, 0.61379775]), 1),\n",
       " (array([0.5650534, 0.4349466]), 1),\n",
       " (array([0.02606091, 0.97393909]), 1),\n",
       " (array([0.19602378, 0.80397622]), 1),\n",
       " (array([0.62039667, 0.37960333]), 0),\n",
       " (array([0.87088449, 0.12911551]), 0),\n",
       " (array([0.04710656, 0.95289344]), 1),\n",
       " (array([0.55488709, 0.44511291]), 0),\n",
       " (array([0.17101614, 0.82898386]), 1),\n",
       " (array([0.80851027, 0.19148973]), 0),\n",
       " (array([0.13167046, 0.86832954]), 1),\n",
       " (array([0.71080775, 0.28919225]), 0),\n",
       " (array([0.76838482, 0.23161518]), 0),\n",
       " (array([0.66414579, 0.33585421]), 0),\n",
       " (array([0.95602514, 0.04397486]), 0),\n",
       " (array([0.07338711, 0.92661289]), 1),\n",
       " (array([0.79964876, 0.20035124]), 0),\n",
       " (array([0.15006654, 0.84993346]), 1),\n",
       " (array([0.923779, 0.076221]), 0),\n",
       " (array([0.45151822, 0.54848178]), 1),\n",
       " (array([0.25069707, 0.74930293]), 1),\n",
       " (array([0.87498855, 0.12501145]), 0),\n",
       " (array([0.00719614, 0.99280386]), 1),\n",
       " (array([0.94576217, 0.05423783]), 0),\n",
       " (array([0.36185367, 0.63814633]), 1),\n",
       " (array([0.20921779, 0.79078221]), 1),\n",
       " (array([0.06372692, 0.93627308]), 1),\n",
       " (array([0.1211368, 0.8788632]), 1),\n",
       " (array([0.912063, 0.087937]), 0),\n",
       " (array([0.10669979, 0.89330021]), 1),\n",
       " (array([0.57946864, 0.42053136]), 0),\n",
       " (array([0.95582103, 0.04417897]), 0),\n",
       " (array([0.82092309, 0.17907691]), 0),\n",
       " (array([0.60125615, 0.39874385]), 1),\n",
       " (array([0.66898223, 0.33101777]), 0),\n",
       " (array([0.5416434, 0.4583566]), 1),\n",
       " (array([0.35537812, 0.64462188]), 0),\n",
       " (array([0.63735019, 0.36264981]), 1),\n",
       " (array([0.07957107, 0.92042893]), 1),\n",
       " (array([0.27307332, 0.72692668]), 0),\n",
       " (array([0.87172377, 0.12827623]), 0),\n",
       " (array([0.88914047, 0.11085953]), 0),\n",
       " (array([0.20178254, 0.79821746]), 1),\n",
       " (array([0.72822887, 0.27177113]), 0),\n",
       " (array([0.30963708, 0.69036292]), 1),\n",
       " (array([0.14377311, 0.85622689]), 1),\n",
       " (array([0.89586273, 0.10413727]), 0),\n",
       " (array([0.39876276, 0.60123724]), 0),\n",
       " (array([0.78645897, 0.21354103]), 0),\n",
       " (array([0.4611, 0.5389]), 0),\n",
       " (array([0.16533094, 0.83466906]), 1),\n",
       " (array([0.05492191, 0.94507809]), 1),\n",
       " (array([0.87848818, 0.12151182]), 0),\n",
       " (array([0.60705309, 0.39294691]), 0),\n",
       " (array([0.84418215, 0.15581785]), 0),\n",
       " (array([0.39381999, 0.60618001]), 1),\n",
       " (array([0.53545179, 0.46454821]), 0),\n",
       " (array([0.67414989, 0.32585011]), 0),\n",
       " (array([0.99390163, 0.00609837]), 0),\n",
       " (array([0.23895075, 0.76104925]), 1),\n",
       " (array([0.13692312, 0.86307688]), 1),\n",
       " (array([0.07596621, 0.92403379]), 1),\n",
       " (array([0.62766857, 0.37233143]), 1),\n",
       " (array([0.9465648, 0.0534352]), 0),\n",
       " (array([0.1394176, 0.8605824]), 1),\n",
       " (array([0.42956877, 0.57043123]), 0),\n",
       " (array([0.47227019, 0.52772981]), 1),\n",
       " (array([0.83875936, 0.16124064]), 0),\n",
       " (array([0.23705953, 0.76294047]), 1),\n",
       " (array([0.01279341, 0.98720659]), 1),\n",
       " (array([0.86406085, 0.13593915]), 1),\n",
       " (array([0.7247544, 0.2752456]), 0),\n",
       " (array([0.85029722, 0.14970278]), 0),\n",
       " (array([0.65448099, 0.34551901]), 0),\n",
       " (array([0.08707485, 0.91292515]), 1),\n",
       " (array([0.16656145, 0.83343855]), 1),\n",
       " (array([0.61356459, 0.38643541]), 0),\n",
       " (array([0.97392982, 0.02607018]), 0),\n",
       " (array([0.42228591, 0.57771409]), 1),\n",
       " (array([0.40527096, 0.59472904]), 0),\n",
       " (array([0.24605654, 0.75394346]), 1),\n",
       " (array([0.98425627, 0.01574373]), 0),\n",
       " (array([0.61140141, 0.38859859]), 1),\n",
       " (array([0.6907083, 0.3092917]), 0),\n",
       " (array([0.24932582, 0.75067418]), 1),\n",
       " (array([0.47950737, 0.52049263]), 1),\n",
       " (array([0.06896163, 0.93103837]), 1),\n",
       " (array([0.08682662, 0.91317338]), 1),\n",
       " (array([0.35065753, 0.64934247]), 1),\n",
       " (array([0.55330717, 0.44669283]), 0),\n",
       " (array([0.9045044, 0.0954956]), 0),\n",
       " (array([0.95773853, 0.04226147]), 0),\n",
       " (array([0.82154421, 0.17845579]), 1),\n",
       " (array([0.77382813, 0.22617187]), 0),\n",
       " (array([0.02655251, 0.97344749]), 1),\n",
       " (array([0.55589155, 0.44410845]), 0),\n",
       " (array([0.06110862, 0.93889138]), 1),\n",
       " (array([0.77059068, 0.22940932]), 0),\n",
       " (array([0.26703942, 0.73296058]), 1),\n",
       " (array([0.12220725, 0.87779275]), 1),\n",
       " (array([0.69808981, 0.30191019]), 1),\n",
       " (array([0.01570983, 0.98429017]), 1),\n",
       " (array([0.02045915, 0.97954085]), 1),\n",
       " (array([0.4474233, 0.5525767]), 1),\n",
       " (array([0.47584453, 0.52415547]), 1),\n",
       " (array([0.80223647, 0.19776353]), 0),\n",
       " (array([0.81791483, 0.18208517]), 0),\n",
       " (array([0.14586335, 0.85413665]), 1),\n",
       " (array([0.15254742, 0.84745258]), 1),\n",
       " (array([0.78951355, 0.21048645]), 0),\n",
       " (array([0.08164935, 0.91835065]), 1),\n",
       " (array([0.17393662, 0.82606338]), 1),\n",
       " (array([0.96211914, 0.03788086]), 0),\n",
       " (array([0.43306101, 0.56693899]), 0),\n",
       " (array([0.95295022, 0.04704978]), 0),\n",
       " (array([0.36231635, 0.63768365]), 1),\n",
       " (array([0.8042585, 0.1957415]), 0),\n",
       " (array([0.08834659, 0.91165341]), 1),\n",
       " (array([0.2444474, 0.7555526]), 1),\n",
       " (array([0.954931, 0.045069]), 0),\n",
       " (array([0.37336246, 0.62663754]), 0),\n",
       " (array([0.61900615, 0.38099385]), 1),\n",
       " (array([0.2879398, 0.7120602]), 1),\n",
       " (array([0.20916468, 0.79083532]), 0),\n",
       " (array([0.032885, 0.967115]), 1),\n",
       " (array([0.22523624, 0.77476376]), 1),\n",
       " (array([0.83730349, 0.16269651]), 0),\n",
       " (array([0.57746824, 0.42253176]), 0),\n",
       " (array([0.95719518, 0.04280482]), 0),\n",
       " (array([0.73191913, 0.26808087]), 0),\n",
       " (array([0.97672702, 0.02327298]), 0),\n",
       " (array([0.89445017, 0.10554983]), 0),\n",
       " (array([0.99672289, 0.00327711]), 0),\n",
       " (array([0.09545484, 0.90454516]), 1),\n",
       " (array([0.01402336, 0.98597664]), 1),\n",
       " (array([0.13209956, 0.86790044]), 1),\n",
       " (array([0.34699917, 0.65300083]), 1),\n",
       " (array([0.56644205, 0.43355795]), 0),\n",
       " (array([0.16932047, 0.83067953]), 1),\n",
       " (array([0.33444165, 0.66555835]), 1),\n",
       " (array([0.73178086, 0.26821914]), 0),\n",
       " (array([0.99756655, 0.00243345]), 0),\n",
       " (array([0.99247293, 0.00752707]), 0),\n",
       " (array([0.15690368, 0.84309632]), 1),\n",
       " (array([0.41501522, 0.58498478]), 0),\n",
       " (array([0.63166823, 0.36833177]), 1),\n",
       " (array([0.48443378, 0.51556622]), 1),\n",
       " (array([0.05150248, 0.94849752]), 1),\n",
       " (array([0.31145178, 0.68854822]), 1),\n",
       " (array([0.17058132, 0.82941868]), 1),\n",
       " (array([0.05615755, 0.94384245]), 1),\n",
       " (array([0.2857618, 0.7142382]), 1),\n",
       " (array([0.37162676, 0.62837324]), 1),\n",
       " (array([0.23786565, 0.76213435]), 1),\n",
       " (array([0.99890663, 0.00109337]), 0),\n",
       " (array([0.05046785, 0.94953215]), 1),\n",
       " (array([0.50997134, 0.49002866]), 1),\n",
       " (array([0.65053986, 0.34946014]), 0),\n",
       " (array([0.8660557, 0.1339443]), 0),\n",
       " (array([0.36743372, 0.63256628]), 0),\n",
       " (array([0.14719826, 0.85280174]), 1),\n",
       " (array([0.69104766, 0.30895234]), 0),\n",
       " (array([0.98964585, 0.01035415]), 0),\n",
       " (array([0.37321349, 0.62678651]), 0),\n",
       " (array([0.23722404, 0.76277596]), 1),\n",
       " (array([0.57378203, 0.42621797]), 0),\n",
       " (array([0.94402477, 0.05597523]), 0),\n",
       " (array([0.19070699, 0.80929301]), 1),\n",
       " (array([0.98282073, 0.01717927]), 0),\n",
       " (array([0.71884149, 0.28115851]), 0),\n",
       " (array([0.43202931, 0.56797069]), 0),\n",
       " (array([0.02712079, 0.97287921]), 1),\n",
       " (array([0.9205871, 0.0794129]), 0),\n",
       " (array([0.78972011, 0.21027989]), 0),\n",
       " (array([0.05693593, 0.94306407]), 1),\n",
       " (array([0.54643392, 0.45356608]), 0),\n",
       " (array([0.86366289, 0.13633711]), 0),\n",
       " (array([0.80037399, 0.19962601]), 1),\n",
       " (array([0.81491129, 0.18508871]), 0),\n",
       " (array([0.18350211, 0.81649789]), 1),\n",
       " (array([0.93627625, 0.06372375]), 0),\n",
       " (array([0.9279151, 0.0720849]), 0),\n",
       " (array([0.83166493, 0.16833507]), 0),\n",
       " (array([0.13954607, 0.86045393]), 1),\n",
       " (array([0.51205292, 0.48794708]), 0),\n",
       " (array([0.33319326, 0.66680674]), 1),\n",
       " (array([0.49496469, 0.50503531]), 0),\n",
       " (array([0.73516485, 0.26483515]), 0),\n",
       " (array([0.98189933, 0.01810067]), 0),\n",
       " (array([0.21292343, 0.78707657]), 1),\n",
       " (array([0.71415694, 0.28584306]), 0),\n",
       " (array([0.9161512, 0.0838488]), 0),\n",
       " (array([0.72196264, 0.27803736]), 0),\n",
       " (array([0.93497447, 0.06502553]), 0),\n",
       " (array([0.39868875, 0.60131125]), 1),\n",
       " (array([0.86797704, 0.13202296]), 0),\n",
       " (array([0.41223105, 0.58776895]), 1),\n",
       " (array([0.89952271, 0.10047729]), 0),\n",
       " (array([0.1638171, 0.8361829]), 1),\n",
       " (array([0.94853112, 0.05146888]), 0),\n",
       " (array([0.85091901, 0.14908099]), 0),\n",
       " (array([0.12435992, 0.87564008]), 1),\n",
       " (array([0.36224462, 0.63775538]), 1),\n",
       " (array([0.40401594, 0.59598406]), 1),\n",
       " (array([0.13193896, 0.86806104]), 1),\n",
       " (array([0.70235824, 0.29764176]), 0),\n",
       " (array([0.09550357, 0.90449643]), 1),\n",
       " (array([0.91815379, 0.08184621]), 0),\n",
       " (array([0.99235909, 0.00764091]), 0),\n",
       " (array([0.55164867, 0.44835133]), 1),\n",
       " (array([0.32385078, 0.67614922]), 1),\n",
       " (array([0.75166226, 0.24833774]), 0),\n",
       " (array([0.99732186, 0.00267814]), 0),\n",
       " (array([0.45512565, 0.54487435]), 0),\n",
       " (array([0.29244334, 0.70755666]), 1),\n",
       " (array([0.08751249, 0.91248751]), 1),\n",
       " (array([0.19223406, 0.80776594]), 1),\n",
       " (array([0.76863121, 0.23136879]), 0),\n",
       " (array([0.21889353, 0.78110647]), 1),\n",
       " (array([0.25293896, 0.74706104]), 1),\n",
       " (array([0.11191673, 0.88808327]), 1),\n",
       " (array([0.98752039, 0.01247961]), 0),\n",
       " (array([0.86029502, 0.13970498]), 0),\n",
       " (array([0.87724002, 0.12275998]), 0),\n",
       " (array([0.14501729, 0.85498271]), 1),\n",
       " (array([0.23063664, 0.76936336]), 0),\n",
       " (array([0.9796237, 0.0203763]), 0),\n",
       " (array([0.99604045, 0.00395955]), 0),\n",
       " (array([0.65959239, 0.34040761]), 0),\n",
       " (array([0.93767824, 0.06232176]), 0),\n",
       " (array([0.19040284, 0.80959716]), 1),\n",
       " (array([0.89607102, 0.10392898]), 0),\n",
       " (array([0.50373516, 0.49626484]), 0),\n",
       " (array([0.20413939, 0.79586061]), 1),\n",
       " (array([0.97172157, 0.02827843]), 0),\n",
       " (array([0.44835045, 0.55164955]), 1),\n",
       " (array([0.39139497, 0.60860503]), 1),\n",
       " (array([0.07823196, 0.92176804]), 1),\n",
       " (array([0.37444056, 0.62555944]), 1),\n",
       " (array([0.00597169, 0.99402831]), 1),\n",
       " (array([0.41325222, 0.58674778]), 1),\n",
       " (array([0.07874549, 0.92125451]), 1),\n",
       " (array([0.34201192, 0.65798808]), 0),\n",
       " (array([0.1571369, 0.8428631]), 1),\n",
       " (array([0.0733675, 0.9266325]), 1),\n",
       " (array([0.84532039, 0.15467961]), 0),\n",
       " (array([0.31939336, 0.68060664]), 1),\n",
       " (array([0.2409767, 0.7590233]), 1),\n",
       " (array([0.42366259, 0.57633741]), 1),\n",
       " (array([0.74533938, 0.25466062]), 0),\n",
       " (array([0.93209349, 0.06790651]), 0),\n",
       " (array([0.0221505, 0.9778495]), 1),\n",
       " (array([0.09738406, 0.90261594]), 1),\n",
       " (array([0.23647642, 0.76352358]), 1),\n",
       " (array([0.24248683, 0.75751317]), 1),\n",
       " (array([0.35543468, 0.64456532]), 1),\n",
       " (array([0.00950454, 0.99049546]), 1),\n",
       " (array([0.90670927, 0.09329073]), 0),\n",
       " (array([0.28098012, 0.71901988]), 1),\n",
       " (array([0.05789767, 0.94210233]), 1),\n",
       " (array([0.9903855, 0.0096145]), 0),\n",
       " (array([0.41276447, 0.58723553]), 0),\n",
       " (array([0.59156521, 0.40843479]), 1),\n",
       " (array([0.5092933, 0.4907067]), 1),\n",
       " (array([0.18921822, 0.81078178]), 1),\n",
       " (array([0.17587769, 0.82412231]), 1),\n",
       " (array([0.25366652, 0.74633348]), 1),\n",
       " (array([0.05698963, 0.94301037]), 1),\n",
       " (array([0.57484085, 0.42515915]), 1),\n",
       " (array([0.00990923, 0.99009077]), 1),\n",
       " (array([0.1192771, 0.8807229]), 1),\n",
       " (array([0.88118582, 0.11881418]), 0),\n",
       " (array([0.63491498, 0.36508502]), 1),\n",
       " (array([0.61477921, 0.38522079]), 1),\n",
       " (array([0.62150727, 0.37849273]), 0),\n",
       " (array([0.93936696, 0.06063304]), 0),\n",
       " (array([0.91585142, 0.08414858]), 0),\n",
       " (array([0.68565268, 0.31434732]), 0),\n",
       " (array([0.96426143, 0.03573857]), 0),\n",
       " (array([0.04900441, 0.95099559]), 1),\n",
       " (array([0.94222806, 0.05777194]), 0),\n",
       " (array([0.77185045, 0.22814955]), 1),\n",
       " (array([0.09117133, 0.90882867]), 1),\n",
       " (array([0.50678652, 0.49321348]), 0),\n",
       " (array([0.75603262, 0.24396738]), 0),\n",
       " (array([0.54841253, 0.45158747]), 0),\n",
       " (array([0.7029704, 0.2970296]), 1),\n",
       " (array([0.91840809, 0.08159191]), 0),\n",
       " (array([0.87370121, 0.12629879]), 0),\n",
       " (array([0.9073797, 0.0926203]), 0),\n",
       " (array([0.76285623, 0.23714377]), 0),\n",
       " (array([0.84986072, 0.15013928]), 0),\n",
       " (array([0.5279878, 0.4720122]), 0),\n",
       " (array([0.27126471, 0.72873529]), 1),\n",
       " (array([0.11520423, 0.88479577]), 1),\n",
       " (array([0.39628491, 0.60371509]), 0),\n",
       " (array([0.31078953, 0.68921047]), 1),\n",
       " (array([0.03276292, 0.96723708]), 1),\n",
       " (array([0.07660411, 0.92339589]), 1),\n",
       " (array([0.45132994, 0.54867006]), 0),\n",
       " (array([0.15361854, 0.84638146]), 1),\n",
       " (array([0.09130947, 0.90869053]), 1),\n",
       " (array([0.84793199, 0.15206801]), 0),\n",
       " (array([0.36109771, 0.63890229]), 1),\n",
       " (array([0.97840206, 0.02159794]), 0),\n",
       " (array([0.37340909, 0.62659091]), 1),\n",
       " (array([0.05088188, 0.94911812]), 1),\n",
       " (array([0.65092209, 0.34907791]), 0),\n",
       " (array([0.17384401, 0.82615599]), 1),\n",
       " (array([0.13058329, 0.86941671]), 1),\n",
       " (array([0.09548497, 0.90451503]), 0),\n",
       " (array([0.2245957, 0.7754043]), 1),\n",
       " (array([0.26602674, 0.73397326]), 1),\n",
       " (array([0.70066416, 0.29933584]), 0),\n",
       " (array([0.98387264, 0.01612736]), 0),\n",
       " (array([0.56177159, 0.43822841]), 0),\n",
       " (array([0.07392886, 0.92607114]), 1),\n",
       " (array([0.11454988, 0.88545012]), 1),\n",
       " (array([0.76568998, 0.23431002]), 1),\n",
       " (array([0.70742476, 0.29257524]), 0),\n",
       " (array([0.15949393, 0.84050607]), 1),\n",
       " (array([0.83826937, 0.16173063]), 0),\n",
       " (array([0.02814228, 0.97185772]), 1),\n",
       " (array([0.51098968, 0.48901032]), 0),\n",
       " (array([0.98224033, 0.01775967]), 0),\n",
       " (array([0.98473432, 0.01526568]), 0),\n",
       " (array([0.30310659, 0.69689341]), 1),\n",
       " (array([0.38885558, 0.61114442]), 1),\n",
       " (array([0.03054372, 0.96945628]), 1),\n",
       " (array([0.33649818, 0.66350182]), 1),\n",
       " (array([0.25853336, 0.74146664]), 0),\n",
       " (array([0.84689798, 0.15310202]), 0),\n",
       " (array([0.42513939, 0.57486061]), 1),\n",
       " (array([0.07298123, 0.92701877]), 1),\n",
       " (array([0.21091935, 0.78908065]), 1),\n",
       " (array([0.31298056, 0.68701944]), 0),\n",
       " (array([0.14942827, 0.85057173]), 1),\n",
       " (array([0.98976916, 0.01023084]), 0),\n",
       " (array([0.78139836, 0.21860164]), 0),\n",
       " (array([0.99679075, 0.00320925]), 0),\n",
       " (array([0.12781075, 0.87218925]), 1),\n",
       " (array([0.83726506, 0.16273494]), 0),\n",
       " (array([0.83626618, 0.16373382]), 0),\n",
       " (array([0.91243716, 0.08756284]), 0),\n",
       " (array([0.68695807, 0.31304193]), 0),\n",
       " (array([0.08722958, 0.91277042]), 1),\n",
       " (array([0.09918658, 0.90081342]), 1),\n",
       " (array([0.09164463, 0.90835537]), 1),\n",
       " (array([0.65770147, 0.34229853]), 0),\n",
       " (array([0.77743912, 0.22256088]), 0),\n",
       " (array([0.8872674, 0.1127326]), 0),\n",
       " (array([0.32119524, 0.67880476]), 0),\n",
       " (array([0.10772042, 0.89227958]), 1),\n",
       " (array([0.00418448, 0.99581552]), 1),\n",
       " (array([0.3683747, 0.6316253]), 1),\n",
       " (array([0.60431865, 0.39568135]), 0),\n",
       " (array([0.47179326, 0.52820674]), 0),\n",
       " (array([0.76224788, 0.23775212]), 0),\n",
       " (array([0.99419186, 0.00580814]), 0),\n",
       " (array([0.87935001, 0.12064999]), 0),\n",
       " (array([0.49390652, 0.50609348]), 0),\n",
       " (array([0.58748458, 0.41251542]), 0),\n",
       " (array([0.4014789, 0.5985211]), 1),\n",
       " (array([0.82738658, 0.17261342]), 0),\n",
       " (array([0.41451851, 0.58548149]), 0),\n",
       " (array([0.22205135, 0.77794865]), 1),\n",
       " (array([0.95628915, 0.04371085]), 0),\n",
       " (array([0.70266105, 0.29733895]), 0),\n",
       " (array([0.50257681, 0.49742319]), 1),\n",
       " (array([0.01693493, 0.98306507]), 1),\n",
       " (array([0.58084739, 0.41915261]), 0),\n",
       " (array([0.46315032, 0.53684968]), 0),\n",
       " (array([0.85538278, 0.14461722]), 0),\n",
       " (array([0.98237703, 0.01762297]), 0),\n",
       " (array([0.44357733, 0.55642267]), 1),\n",
       " (array([0.84157298, 0.15842702]), 0),\n",
       " (array([0.86946955, 0.13053045]), 0),\n",
       " (array([0.04349039, 0.95650961]), 1),\n",
       " (array([0.44273347, 0.55726653]), 0),\n",
       " (array([0.86061694, 0.13938306]), 0),\n",
       " (array([0.9742747, 0.0257253]), 0),\n",
       " (array([0.1545825, 0.8454175]), 1),\n",
       " (array([0.04404125, 0.95595875]), 1),\n",
       " (array([0.1324185, 0.8675815]), 1),\n",
       " (array([0.74390877, 0.25609123]), 0),\n",
       " (array([0.14917275, 0.85082725]), 1),\n",
       " (array([0.87094217, 0.12905783]), 0),\n",
       " (array([0.8295523, 0.1704477]), 0),\n",
       " (array([0.60242428, 0.39757572]), 0),\n",
       " (array([0.83737843, 0.16262157]), 0),\n",
       " (array([0.64478851, 0.35521149]), 1),\n",
       " (array([0.9431251, 0.0568749]), 0),\n",
       " (array([0.81238718, 0.18761282]), 0),\n",
       " (array([0.96749558, 0.03250442]), 0),\n",
       " (array([0.60936794, 0.39063206]), 0),\n",
       " (array([0.13302845, 0.86697155]), 1),\n",
       " (array([0.85917057, 0.14082943]), 0),\n",
       " (array([0.04380403, 0.95619597]), 1),\n",
       " (array([0.83935991, 0.16064009]), 0),\n",
       " (array([0.96676137, 0.03323863]), 0),\n",
       " (array([0.24047944, 0.75952056]), 1),\n",
       " (array([0.83584564, 0.16415436]), 0),\n",
       " (array([0.65813983, 0.34186017]), 0),\n",
       " (array([0.82700343, 0.17299657]), 0),\n",
       " (array([0.57337676, 0.42662324]), 1),\n",
       " (array([0.88681033, 0.11318967]), 0),\n",
       " (array([0.10944229, 0.89055771]), 1),\n",
       " (array([0.51350322, 0.48649678]), 1),\n",
       " (array([0.79575981, 0.20424019]), 0),\n",
       " (array([0.22145408, 0.77854592]), 1),\n",
       " (array([0.7976265, 0.2023735]), 0),\n",
       " (array([0.86925196, 0.13074804]), 0),\n",
       " (array([0.02301688, 0.97698312]), 1),\n",
       " (array([0.73022287, 0.26977713]), 0),\n",
       " (array([0.28292396, 0.71707604]), 1),\n",
       " (array([0.99046716, 0.00953284]), 0),\n",
       " (array([0.12876492, 0.87123508]), 1),\n",
       " (array([0.52611477, 0.47388523]), 0),\n",
       " (array([0.09201658, 0.90798342]), 1),\n",
       " (array([0.25258957, 0.74741043]), 1),\n",
       " (array([0.71391459, 0.28608541]), 0),\n",
       " (array([0.18394264, 0.81605736]), 1),\n",
       " (array([0.89083644, 0.10916356]), 0),\n",
       " (array([0.80309412, 0.19690588]), 0),\n",
       " (array([0.25590761, 0.74409239]), 1),\n",
       " (array([0.96640878, 0.03359122]), 0),\n",
       " (array([0.99239714, 0.00760286]), 0),\n",
       " (array([0.18547888, 0.81452112]), 1),\n",
       " (array([0.90599752, 0.09400248]), 0),\n",
       " (array([0.05621421, 0.94378579]), 1),\n",
       " (array([0.43487691, 0.56512309]), 0),\n",
       " (array([0.31305715, 0.68694285]), 1),\n",
       " (array([0.88288834, 0.11711166]), 0),\n",
       " (array([0.79826405, 0.20173595]), 0),\n",
       " (array([0.95514684, 0.04485316]), 0),\n",
       " (array([0.32484936, 0.67515064]), 1),\n",
       " (array([0.13497176, 0.86502824]), 1),\n",
       " (array([0.20308966, 0.79691034]), 1),\n",
       " (array([0.80057024, 0.19942976]), 0),\n",
       " (array([0.16538136, 0.83461864]), 1),\n",
       " (array([0.9327304, 0.0672696]), 0),\n",
       " (array([0.10364115, 0.89635885]), 1),\n",
       " (array([0.41395655, 0.58604345]), 1),\n",
       " (array([0.90860669, 0.09139331]), 0),\n",
       " (array([0.95299035, 0.04700965]), 0),\n",
       " (array([0.25424902, 0.74575098]), 1),\n",
       " (array([0.26952856, 0.73047144]), 1),\n",
       " (array([0.74285442, 0.25714558]), 0),\n",
       " (array([0.97943642, 0.02056358]), 0),\n",
       " (array([0.89630028, 0.10369972]), 0),\n",
       " (array([0.64858836, 0.35141164]), 1),\n",
       " (array([0.82262134, 0.17737866]), 0),\n",
       " (array([0.72352726, 0.27647274]), 0),\n",
       " (array([0.0759605, 0.9240395]), 1),\n",
       " (array([0.77360054, 0.22639946]), 0),\n",
       " (array([0.16083875, 0.83916125]), 1),\n",
       " (array([0.68924952, 0.31075048]), 0),\n",
       " (array([0.09536471, 0.90463529]), 1),\n",
       " (array([0.09018437, 0.90981563]), 1),\n",
       " (array([0.21093561, 0.78906439]), 1),\n",
       " (array([0.17872879, 0.82127121]), 1),\n",
       " (array([0.98543863, 0.01456137]), 0),\n",
       " (array([0.50563207, 0.49436793]), 0),\n",
       " (array([0.89311944, 0.10688056]), 0),\n",
       " (array([0.4751639, 0.5248361]), 1),\n",
       " (array([0.90559094, 0.09440906]), 0),\n",
       " (array([0.54407038, 0.45592962]), 0),\n",
       " (array([0.60385427, 0.39614573]), 0),\n",
       " (array([0.96140318, 0.03859682]), 0),\n",
       " (array([0.99116462, 0.00883538]), 0),\n",
       " (array([0.17960942, 0.82039058]), 1),\n",
       " (array([0.6698702, 0.3301298]), 0),\n",
       " (array([0.93262015, 0.06737985]), 0),\n",
       " (array([0.13442394, 0.86557606]), 1),\n",
       " (array([0.18531993, 0.81468007]), 1),\n",
       " (array([0.83003199, 0.16996801]), 0),\n",
       " (array([0.69229438, 0.30770562]), 1),\n",
       " (array([0.33770613, 0.66229387]), 0),\n",
       " (array([0.08939206, 0.91060794]), 1),\n",
       " (array([0.28509014, 0.71490986]), 1),\n",
       " (array([0.87654212, 0.12345788]), 0),\n",
       " (array([0.97470377, 0.02529623]), 0),\n",
       " (array([0.18156231, 0.81843769]), 1),\n",
       " (array([0.98509903, 0.01490097]), 0),\n",
       " (array([0.34163925, 0.65836075]), 1),\n",
       " (array([0.13315849, 0.86684151]), 1),\n",
       " (array([0.52147944, 0.47852056]), 1),\n",
       " (array([0.60567963, 0.39432037]), 0),\n",
       " (array([0.20711286, 0.79288714]), 1),\n",
       " (array([0.88341058, 0.11658942]), 0),\n",
       " (array([0.74413348, 0.25586652]), 0),\n",
       " (array([0.07420187, 0.92579813]), 1),\n",
       " (array([0.10336365, 0.89663635]), 1),\n",
       " (array([0.73233984, 0.26766016]), 0),\n",
       " (array([0.43040086, 0.56959914]), 1),\n",
       " (array([0.51920526, 0.48079474]), 1),\n",
       " (array([0.77181803, 0.22818197]), 0),\n",
       " (array([0.54175436, 0.45824564]), 0),\n",
       " (array([0.69520034, 0.30479966]), 0),\n",
       " (array([0.79607494, 0.20392506]), 0),\n",
       " (array([0.37894433, 0.62105567]), 1),\n",
       " (array([0.76318238, 0.23681762]), 0),\n",
       " (array([0.27715092, 0.72284908]), 1),\n",
       " (array([0.39767273, 0.60232727]), 1),\n",
       " (array([0.86709782, 0.13290218]), 0),\n",
       " (array([0.07340731, 0.92659269]), 1),\n",
       " (array([0.07192092, 0.92807908]), 1),\n",
       " (array([0.41476603, 0.58523397]), 1),\n",
       " (array([0.7755689, 0.2244311]), 0),\n",
       " (array([0.81972541, 0.18027459]), 0),\n",
       " (array([0.18475095, 0.81524905]), 1),\n",
       " (array([0.85927496, 0.14072504]), 0),\n",
       " (array([0.48468663, 0.51531337]), 0),\n",
       " (array([0.41019212, 0.58980788]), 1),\n",
       " (array([0.8923906, 0.1076094]), 0),\n",
       " (array([0.21958539, 0.78041461]), 0),\n",
       " (array([0.25088444, 0.74911556]), 1),\n",
       " (array([0.9884881, 0.0115119]), 0),\n",
       " (array([0.6588299, 0.3411701]), 0),\n",
       " (array([0.66088539, 0.33911461]), 0),\n",
       " (array([0.90912461, 0.09087539]), 0),\n",
       " (array([0.0915743, 0.9084257]), 1),\n",
       " (array([0.96301872, 0.03698128]), 0),\n",
       " (array([0.83133014, 0.16866986]), 0),\n",
       " (array([0.00980097, 0.99019903]), 1),\n",
       " (array([0.18192188, 0.81807812]), 1),\n",
       " (array([0.19932483, 0.80067517]), 1),\n",
       " (array([0.35563988, 0.64436012]), 0),\n",
       " (array([0.01187782, 0.98812218]), 1),\n",
       " (array([0.83813346, 0.16186654]), 0),\n",
       " (array([0.23642687, 0.76357313]), 1),\n",
       " (array([0.48460429, 0.51539571]), 0),\n",
       " (array([0.87985704, 0.12014296]), 0),\n",
       " (array([0.1374001, 0.8625999]), 1),\n",
       " (array([0.21895012, 0.78104988]), 1),\n",
       " (array([0.98772213, 0.01227787]), 0),\n",
       " (array([0.96539561, 0.03460439]), 0),\n",
       " (array([0.05558094, 0.94441906]), 1),\n",
       " (array([0.90004367, 0.09995633]), 0),\n",
       " (array([0.05062645, 0.94937355]), 1),\n",
       " (array([0.12577621, 0.87422379]), 1),\n",
       " (array([0.05223934, 0.94776066]), 1),\n",
       " (array([0.96202768, 0.03797232]), 0),\n",
       " (array([0.74073329, 0.25926671]), 0),\n",
       " (array([0.50231106, 0.49768894]), 0),\n",
       " (array([0.85377219, 0.14622781]), 0),\n",
       " (array([0.93598702, 0.06401298]), 0),\n",
       " (array([0.60187882, 0.39812118]), 0),\n",
       " (array([0.17640671, 0.82359329]), 1),\n",
       " (array([0.05726185, 0.94273815]), 1),\n",
       " (array([0.99027145, 0.00972855]), 0),\n",
       " (array([0.2799401, 0.7200599]), 0),\n",
       " (array([0.14979951, 0.85020049]), 1),\n",
       " (array([0.22239956, 0.77760044]), 1),\n",
       " (array([0.27655607, 0.72344393]), 1),\n",
       " (array([0.92144311, 0.07855689]), 0),\n",
       " (array([0.05799982, 0.94200018]), 1),\n",
       " (array([0.0768419, 0.9231581]), 1),\n",
       " (array([0.33524341, 0.66475659]), 1),\n",
       " (array([0.21944195, 0.78055805]), 1),\n",
       " (array([0.08698696, 0.91301304]), 1),\n",
       " (array([0.0220591, 0.9779409]), 1),\n",
       " (array([0.69614505, 0.30385495]), 0),\n",
       " (array([0.74868176, 0.25131824]), 0),\n",
       " (array([0.08187005, 0.91812995]), 1),\n",
       " (array([0.66866569, 0.33133431]), 0),\n",
       " (array([0.89124463, 0.10875537]), 0),\n",
       " (array([0.17917628, 0.82082372]), 1),\n",
       " (array([0.90844868, 0.09155132]), 0),\n",
       " (array([0.52109943, 0.47890057]), 0),\n",
       " (array([0.08035892, 0.91964108]), 1),\n",
       " (array([0.83538243, 0.16461757]), 0),\n",
       " (array([0.26678229, 0.73321771]), 1),\n",
       " (array([0.51150106, 0.48849894]), 0),\n",
       " (array([0.98026528, 0.01973472]), 0),\n",
       " (array([0.88693516, 0.11306484]), 0),\n",
       " (array([0.13923447, 0.86076553]), 1),\n",
       " (array([0.90316749, 0.09683251]), 0),\n",
       " (array([0.4570923, 0.5429077]), 1),\n",
       " (array([0.66008257, 0.33991743]), 0),\n",
       " (array([0.2537339, 0.7462661]), 1),\n",
       " (array([0.85511796, 0.14488204]), 0),\n",
       " (array([0.86453758, 0.13546242]), 0),\n",
       " (array([0.24085354, 0.75914646]), 1),\n",
       " (array([0.8121776, 0.1878224]), 0),\n",
       " (array([0.98411535, 0.01588465]), 0),\n",
       " (array([0.35815891, 0.64184109]), 0),\n",
       " (array([0.07923772, 0.92076228]), 1),\n",
       " (array([0.30310976, 0.69689024]), 1),\n",
       " (array([0.15739218, 0.84260782]), 1),\n",
       " (array([0.511313, 0.488687]), 1),\n",
       " (array([0.48773145, 0.51226855]), 0),\n",
       " (array([0.60852765, 0.39147235]), 1),\n",
       " (array([0.89101609, 0.10898391]), 0),\n",
       " (array([0.82711706, 0.17288294]), 0),\n",
       " (array([0.88081975, 0.11918025]), 0),\n",
       " (array([0.23212631, 0.76787369]), 1),\n",
       " (array([0.96150585, 0.03849415]), 0),\n",
       " (array([0.56516331, 0.43483669]), 1),\n",
       " (array([0.01001094, 0.98998906]), 1),\n",
       " (array([0.96785659, 0.03214341]), 0),\n",
       " (array([0.41857415, 0.58142585]), 0),\n",
       " (array([0.14904311, 0.85095689]), 1),\n",
       " (array([0.29200353, 0.70799647]), 0),\n",
       " (array([0.19864289, 0.80135711]), 1),\n",
       " (array([0.05731521, 0.94268479]), 1),\n",
       " (array([0.58843645, 0.41156355]), 1),\n",
       " (array([0.92144908, 0.07855092]), 0),\n",
       " (array([0.48830825, 0.51169175]), 0),\n",
       " (array([0.40190014, 0.59809986]), 0),\n",
       " (array([0.92582307, 0.07417693]), 0),\n",
       " (array([0.20548723, 0.79451277]), 1),\n",
       " (array([0.18468433, 0.81531567]), 1),\n",
       " (array([0.07053515, 0.92946485]), 1),\n",
       " (array([0.37551969, 0.62448031]), 1),\n",
       " (array([0.81828779, 0.18171221]), 0),\n",
       " (array([0.02436591, 0.97563409]), 1),\n",
       " (array([0.60400404, 0.39599596]), 0),\n",
       " (array([0.16383143, 0.83616857]), 1),\n",
       " (array([0.88634938, 0.11365062]), 0),\n",
       " (array([0.71428492, 0.28571508]), 0),\n",
       " (array([0.61574561, 0.38425439]), 0),\n",
       " (array([0.88389084, 0.11610916]), 0),\n",
       " (array([0.32243449, 0.67756551]), 1),\n",
       " (array([0.84714029, 0.15285971]), 0),\n",
       " (array([0.76460663, 0.23539337]), 0),\n",
       " (array([0.92044844, 0.07955156]), 0),\n",
       " (array([0.8676126, 0.1323874]), 0),\n",
       " (array([0.10111672, 0.89888328]), 1),\n",
       " (array([0.36633838, 0.63366162]), 1),\n",
       " (array([0.07407557, 0.92592443]), 1),\n",
       " (array([0.33421267, 0.66578733]), 1),\n",
       " (array([0.40186219, 0.59813781]), 1),\n",
       " (array([0.92847559, 0.07152441]), 0),\n",
       " (array([0.79104644, 0.20895356]), 0),\n",
       " (array([0.88393031, 0.11606969]), 0),\n",
       " (array([0.89667686, 0.10332314]), 0),\n",
       " (array([0.96841987, 0.03158013]), 0),\n",
       " (array([0.25381603, 0.74618397]), 1),\n",
       " (array([0.22603696, 0.77396304]), 1),\n",
       " (array([0.23347049, 0.76652951]), 1),\n",
       " (array([0.28432186, 0.71567814]), 0),\n",
       " (array([0.60695683, 0.39304317]), 0),\n",
       " (array([0.90992817, 0.09007183]), 0),\n",
       " (array([0.80203549, 0.19796451]), 0),\n",
       " (array([0.15649258, 0.84350742]), 1),\n",
       " (array([0.09643675, 0.90356325]), 1),\n",
       " (array([0.12190635, 0.87809365]), 1),\n",
       " (array([0.11165861, 0.88834139]), 1),\n",
       " (array([0.05105246, 0.94894754]), 1),\n",
       " (array([0.2680374, 0.7319626]), 1),\n",
       " (array([0.71444456, 0.28555544]), 0),\n",
       " (array([0.31915043, 0.68084957]), 1),\n",
       " (array([0.8223807, 0.1776193]), 0),\n",
       " (array([0.9513979, 0.0486021]), 0),\n",
       " (array([0.84964037, 0.15035963]), 0),\n",
       " (array([0.1255996, 0.8744004]), 1),\n",
       " (array([0.17745705, 0.82254295]), 1),\n",
       " (array([0.41574723, 0.58425277]), 0),\n",
       " (array([0.17986178, 0.82013822]), 1),\n",
       " (array([0.83839561, 0.16160439]), 0),\n",
       " (array([0.31829437, 0.68170563]), 1),\n",
       " (array([0.93823843, 0.06176157]), 0),\n",
       " (array([0.35782018, 0.64217982]), 1),\n",
       " (array([0.042085, 0.957915]), 1),\n",
       " (array([0.82870463, 0.17129537]), 0),\n",
       " (array([0.69684524, 0.30315476]), 0),\n",
       " (array([0.70161659, 0.29838341]), 0),\n",
       " (array([0.47252922, 0.52747078]), 0),\n",
       " (array([0.07821104, 0.92178896]), 1),\n",
       " (array([0.78621177, 0.21378823]), 0),\n",
       " (array([0.99524746, 0.00475254]), 0),\n",
       " (array([0.10569764, 0.89430236]), 1),\n",
       " (array([0.03056844, 0.96943156]), 1),\n",
       " (array([0.96375192, 0.03624808]), 0),\n",
       " (array([0.99607698, 0.00392302]), 0),\n",
       " (array([0.4892365, 0.5107635]), 0),\n",
       " (array([0.67747797, 0.32252203]), 1),\n",
       " (array([0.88023108, 0.11976892]), 0),\n",
       " (array([0.88140189, 0.11859811]), 0),\n",
       " (array([0.92510738, 0.07489262]), 0),\n",
       " (array([0.52402278, 0.47597722]), 0),\n",
       " (array([0.31419535, 0.68580465]), 1),\n",
       " (array([0.25320493, 0.74679507]), 1),\n",
       " (array([0.46773526, 0.53226474]), 1),\n",
       " (array([0.79501802, 0.20498198]), 1),\n",
       " (array([0.95270658, 0.04729342]), 0),\n",
       " (array([0.7531734, 0.2468266]), 0),\n",
       " (array([0.46882819, 0.53117181]), 1),\n",
       " (array([0.98448274, 0.01551726]), 0),\n",
       " (array([0.25875302, 0.74124698]), 1),\n",
       " (array([0.12797397, 0.87202603]), 1),\n",
       " (array([0.3588288, 0.6411712]), 0),\n",
       " (array([0.74858543, 0.25141457]), 0),\n",
       " (array([0.19586069, 0.80413931]), 1),\n",
       " (array([0.37958653, 0.62041347]), 1),\n",
       " (array([0.82941292, 0.17058708]), 0),\n",
       " (array([0.71242309, 0.28757691]), 0),\n",
       " (array([0.21952632, 0.78047368]), 1),\n",
       " (array([0.85318774, 0.14681226]), 0),\n",
       " (array([0.94906387, 0.05093613]), 0),\n",
       " (array([0.62718231, 0.37281769]), 0),\n",
       " (array([0.37114323, 0.62885677]), 1),\n",
       " (array([0.1791624, 0.8208376]), 1),\n",
       " (array([0.06857174, 0.93142826]), 1),\n",
       " (array([0.24510258, 0.75489742]), 1),\n",
       " (array([0.77962017, 0.22037983]), 0),\n",
       " (array([0.95752311, 0.04247689]), 0),\n",
       " (array([0.97919905, 0.02080095]), 0),\n",
       " (array([0.64892544, 0.35107456]), 0),\n",
       " (array([0.83787584, 0.16212416]), 1),\n",
       " (array([0.85313639, 0.14686361]), 0),\n",
       " (array([0.10416528, 0.89583472]), 1),\n",
       " (array([0.3797931, 0.6202069]), 0),\n",
       " (array([0.99227684, 0.00772316]), 0),\n",
       " (array([0.90098074, 0.09901926]), 0),\n",
       " (array([0.19861883, 0.80138117]), 1),\n",
       " (array([0.19744865, 0.80255135]), 1),\n",
       " (array([0.96955868, 0.03044132]), 0),\n",
       " (array([0.43642665, 0.56357335]), 0),\n",
       " (array([0.60676227, 0.39323773]), 0),\n",
       " (array([0.50716537, 0.49283463]), 1),\n",
       " (array([0.0504066, 0.9495934]), 1),\n",
       " (array([0.50496534, 0.49503466]), 0),\n",
       " (array([0.46621059, 0.53378941]), 0),\n",
       " (array([0.45905451, 0.54094549]), 1),\n",
       " (array([0.14256874, 0.85743126]), 1),\n",
       " (array([0.37777642, 0.62222358]), 0),\n",
       " (array([0.73520225, 0.26479775]), 1),\n",
       " (array([0.03201123, 0.96798877]), 1),\n",
       " (array([0.73572933, 0.26427067]), 0),\n",
       " (array([0.14025916, 0.85974084]), 1),\n",
       " (array([0.45060441, 0.54939559]), 1),\n",
       " (array([0.91723128, 0.08276872]), 0),\n",
       " (array([0.92751456, 0.07248544]), 0),\n",
       " (array([0.9958098, 0.0041902]), 0),\n",
       " (array([0.53866421, 0.46133579]), 0),\n",
       " ...]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(clf.predict_proba(x_test_m), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2cec35e3-6c73-4f6f-bc92-6485cb089d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Train Accuracy: 0.9330\n",
      "Logistic Regression Test Accuracy: 0.8831\n"
     ]
    }
   ],
   "source": [
    "#Linear regression\n",
    "accuracy_lr_train = clf.score(x_train_m, y_train)\n",
    "accuracy_lr_test = clf.score(x_test_m, y_test)\n",
    "\n",
    "\n",
    "print(f'Logistic Regression Train Accuracy: {accuracy_lr_train:.4f}')\n",
    "print(f'Logistic Regression Test Accuracy: {accuracy_lr_test:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30aacd13-6e37-46e5-b12b-d098c189bcd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Train Accuracy: 0.9086\n",
      "Naive Bayes Test Accuracy: 0.8298\n"
     ]
    }
   ],
   "source": [
    "# Train Naive Bayes model\n",
    "clf_nb = MultinomialNB().fit(pipe.transform(x_train_corpus), y_train)\n",
    "\n",
    "accuracy_nb_train = clf_nb.score(pipe.transform(x_train_corpus), y_train)\n",
    "accuracy_nb_test = clf_nb.score(pipe.transform(x_test_corpus), y_test)\n",
    "\n",
    "print(f'Naive Bayes Train Accuracy: {accuracy_nb_train:.4f}')\n",
    "print(f'Naive Bayes Test Accuracy: {accuracy_nb_test:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8eb671b3-e9cb-49a3-b931-d42a34bfbc82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy  Naive Bayes  Regression  PyTorch\n",
      "0    Train       0.9086      0.9330    0.946\n",
      "1     Test       0.8298      0.8831    0.725\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "naive_bayes_train_accuracy = 0.9086\n",
    "naive_bayes_test_accuracy = 0.8298\n",
    "logistic_regression_train_accuracy = 0.9330\n",
    "logistic_regression_test_accuracy = 0.8831\n",
    "deep_learning_train_accuracy = 0.9460\n",
    "deep_learning_test_accuracy = 0.7250\n",
    "\n",
    "results = {\n",
    "    \"Accuracy\": [\"Train\", \"Test\"],\n",
    "    \"Naive Bayes\": [\n",
    "        naive_bayes_train_accuracy, \n",
    "        naive_bayes_test_accuracy\n",
    "    ],\n",
    "    \"Regression\": [\n",
    "        logistic_regression_train_accuracy,\n",
    "        logistic_regression_test_accuracy\n",
    "    ],\n",
    "    \"PyTorch\": [\n",
    "        deep_learning_train_accuracy,\n",
    "        deep_learning_test_accuracy\n",
    "    ],\n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a68f614",
   "metadata": {},
   "source": [
    "## 6.2.\tUsing the same data from HW-4, exercise 4, create a DL network using Conv1D’s to perform document classification.  Do not use Embeddings yet. Try at least two network architectures. You will present the result in a table with accuracy for columns `Naïve Bayes`, `XGBoost` and `Deep Learning #1` and `Deep Learning #2`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3cb32c-7188-4c42-92b0-531c34abeca7",
   "metadata": {},
   "source": [
    "Instead of using TFIDF or count vectors, we will use `sentence transformers`, which compute an embedding vector for the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f66c4c89-d7e9-4281-8bc1-b73481bc7db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4cb43e8-4940-4f4a-8c5a-bcf084350c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Owner\\anaconda3\\envs\\torch\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Owner\\anaconda3\\envs\\torch\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:435: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import reuters\n",
    "from keras.utils import to_categorical\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# Load dataset\n",
    "(train_data, train_labels), (test_data, test_labels) = reuters.load_data()\n",
    "w2i = reuters.get_word_index()\n",
    "i2w = dict([(value, key) for (key, value) in w2i.items()])\n",
    "\n",
    "# Convert to text\n",
    "train_data_text = [' '.join([i2w.get(i - 3, '?') for i in s]) for s in train_data]\n",
    "test_data_text = [' '.join([i2w.get(i - 3, '?') for i in s]) for s in test_data]\n",
    "\n",
    "# SentenceTransformer\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\", device='cuda')\n",
    "x_train_e = model.encode(train_data_text)\n",
    "x_test_e = model.encode(test_data_text)\n",
    "\n",
    "x_train_e.shape, x_test_e.shape  # Ensure the shapes are correct\n",
    "\n",
    "num_classes = max(train_labels) + 1\n",
    "y_train = to_categorical(train_labels, num_classes)\n",
    "y_test = to_categorical(test_labels, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b474468-8a2e-4001-bd01-ad2501fac834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy: 0.7471\n"
     ]
    }
   ],
   "source": [
    "# Train Naive Bayes model\n",
    "nb_model = GaussianNB()\n",
    "nb_model.fit(x_train_e, train_labels)\n",
    "nb_pred = nb_model.predict(x_test_e)\n",
    "\n",
    "# accuracy\n",
    "accuracy_nb = accuracy_score(test_labels, nb_pred)\n",
    "\n",
    "print(f\"Naive Bayes Accuracy: {accuracy_nb:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb68d341-a21c-4568-8fa3-aa4808951084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d (Conv1D)             (None, 380, 128)          768       \n",
      "                                                                 \n",
      " global_max_pooling1d (Globa  (None, 128)              0         \n",
      " lMaxPooling1D)                                                  \n",
      "                                                                 \n",
      " dense (Dense)               (None, 46)                5934      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,702\n",
      "Trainable params: 6,702\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D, Dense\n",
    "\n",
    "# copied from last quarter\n",
    "model1 = Sequential([\n",
    "    Conv1D(128, 5, activation='relu', input_shape=(x_train_e.shape[1], 1)),\n",
    "    GlobalMaxPooling1D(),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "model1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model1.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d35a2a5e-26f6-48b3-89d8-0e3538d8f6d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_1 (Conv1D)           (None, 380, 64)           384       \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 376, 64)           20544     \n",
      "                                                                 \n",
      " global_max_pooling1d_1 (Glo  (None, 64)               0         \n",
      " balMaxPooling1D)                                                \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               8320      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 46)                5934      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 35,182\n",
      "Trainable params: 35,182\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#copied from last quarter\n",
    "model2 = Sequential([\n",
    "    Conv1D(64, 5, activation='relu', input_shape=(x_train_e.shape[1], 1)),\n",
    "    Conv1D(64, 5, activation='relu'),\n",
    "    GlobalMaxPooling1D(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "model2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model2.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea99212f-f9f3-4d63-aec9-e04be81def23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "225/225 [==============================] - 9s 37ms/step - loss: 2.7790 - accuracy: 0.3445 - val_loss: 2.3992 - val_accuracy: 0.3450\n",
      "Epoch 2/10\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 2.3981 - accuracy: 0.3534 - val_loss: 2.3940 - val_accuracy: 0.3450\n",
      "Epoch 3/10\n",
      "225/225 [==============================] - 6s 28ms/step - loss: 2.3894 - accuracy: 0.3534 - val_loss: 2.3854 - val_accuracy: 0.3450\n",
      "Epoch 4/10\n",
      "225/225 [==============================] - 5s 21ms/step - loss: 2.3831 - accuracy: 0.3534 - val_loss: 2.3818 - val_accuracy: 0.3450\n",
      "Epoch 5/10\n",
      "225/225 [==============================] - 6s 29ms/step - loss: 2.3755 - accuracy: 0.3534 - val_loss: 2.3749 - val_accuracy: 0.3450\n",
      "Epoch 6/10\n",
      "225/225 [==============================] - 9s 38ms/step - loss: 2.3667 - accuracy: 0.3534 - val_loss: 2.3614 - val_accuracy: 0.3450\n",
      "Epoch 7/10\n",
      "225/225 [==============================] - 7s 33ms/step - loss: 2.3568 - accuracy: 0.3534 - val_loss: 2.3530 - val_accuracy: 0.3450\n",
      "Epoch 8/10\n",
      "225/225 [==============================] - 9s 41ms/step - loss: 2.3454 - accuracy: 0.3531 - val_loss: 2.3361 - val_accuracy: 0.3456\n",
      "Epoch 9/10\n",
      "225/225 [==============================] - 9s 41ms/step - loss: 2.3323 - accuracy: 0.3538 - val_loss: 2.3292 - val_accuracy: 0.3456\n",
      "Epoch 10/10\n",
      "225/225 [==============================] - 6s 25ms/step - loss: 2.3192 - accuracy: 0.3552 - val_loss: 2.3160 - val_accuracy: 0.3467\n",
      "Epoch 1/10\n",
      "225/225 [==============================] - 15s 63ms/step - loss: 2.5331 - accuracy: 0.3505 - val_loss: 2.3688 - val_accuracy: 0.3450\n",
      "Epoch 2/10\n",
      "225/225 [==============================] - 10s 44ms/step - loss: 2.2799 - accuracy: 0.3839 - val_loss: 2.1934 - val_accuracy: 0.4213\n",
      "Epoch 3/10\n",
      "225/225 [==============================] - 15s 65ms/step - loss: 2.1597 - accuracy: 0.4384 - val_loss: 2.1408 - val_accuracy: 0.4485\n",
      "Epoch 4/10\n",
      "225/225 [==============================] - 16s 72ms/step - loss: 2.1027 - accuracy: 0.4553 - val_loss: 2.1109 - val_accuracy: 0.4424\n",
      "Epoch 5/10\n",
      "225/225 [==============================] - 16s 72ms/step - loss: 2.0639 - accuracy: 0.4629 - val_loss: 2.0654 - val_accuracy: 0.4485\n",
      "Epoch 6/10\n",
      "225/225 [==============================] - 16s 72ms/step - loss: 2.0259 - accuracy: 0.4720 - val_loss: 2.0194 - val_accuracy: 0.4752\n",
      "Epoch 7/10\n",
      "225/225 [==============================] - 16s 72ms/step - loss: 1.9870 - accuracy: 0.4852 - val_loss: 2.0062 - val_accuracy: 0.4624\n",
      "Epoch 8/10\n",
      "225/225 [==============================] - 17s 74ms/step - loss: 1.9600 - accuracy: 0.4952 - val_loss: 1.9464 - val_accuracy: 0.4992\n",
      "Epoch 9/10\n",
      "225/225 [==============================] - 16s 73ms/step - loss: 1.9275 - accuracy: 0.5052 - val_loss: 1.9437 - val_accuracy: 0.5042\n",
      "Epoch 10/10\n",
      "225/225 [==============================] - 15s 66ms/step - loss: 1.9012 - accuracy: 0.5130 - val_loss: 1.9096 - val_accuracy: 0.5103\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 2.3225 - accuracy: 0.3629\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 1.9414 - accuracy: 0.5071\n",
      "Deep Learning #1 Accuracy: 0.3629\n",
      "Deep Learning #2 Accuracy: 0.5071\n"
     ]
    }
   ],
   "source": [
    "# Reshape input\n",
    "x_train_e = x_train_e[..., None]\n",
    "x_test_e = x_test_e[..., None]\n",
    "\n",
    "# Train the models\n",
    "history1 = model1.fit(x_train_e, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "history2 = model2.fit(x_train_e, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate the models\n",
    "loss1, accuracy1 = model1.evaluate(x_test_e, y_test)\n",
    "loss2, accuracy2 = model2.evaluate(x_test_e, y_test)\n",
    "\n",
    "print(f\"Deep Learning #1 Accuracy: {accuracy1:.4f}\")\n",
    "print(f\"Deep Learning #2 Accuracy: {accuracy2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6ce1487-a235-435c-8a6b-79b524a6ac5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Accuracy: 0.7734\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Train\n",
    "xgb_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
    "xgb_model.fit(x_train_e, train_labels)\n",
    "\n",
    "xgb_pred = xgb_model.predict(x_test_e)\n",
    "\n",
    "# Evaluate\n",
    "accuracy_xgb = accuracy_score(test_labels, xgb_pred)\n",
    "print(f\"XGBoost Accuracy: {accuracy_xgb:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffa70180-f7cf-4fa0-a552-e8d73a5bb867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Model  Accuracy\n",
      "0       Naïve Bayes    0.7471\n",
      "1           XGBoost    0.9000\n",
      "2  Deep Learning #1    0.3629\n",
      "3  Deep Learning #2    0.5071\n"
     ]
    }
   ],
   "source": [
    "accuracy_xgb = 0.90  \n",
    "accuracy_nb = 0.7471  \n",
    "accuracy1 = 0.3629    \n",
    "accuracy2 = 0.5071    \n",
    "\n",
    "# Results table\n",
    "results = {\n",
    "    \"Model\": [\"Naïve Bayes\", \"XGBoost\", \"Deep Learning #1\", \"Deep Learning #2\"],\n",
    "    \"Accuracy\": [accuracy_nb, accuracy_xgb, accuracy1, accuracy2]\n",
    "}\n",
    "\n",
    "import pandas as pd\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671d9215-78a4-47f9-9f73-7563cc236de8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
