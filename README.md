# Natural Language Processing
Contains all Natural Language Processing(NLP) with Deep Learning labs done in ECEN 523. All done with _**PyTorch**_.
* **Lab 1** defines terms (such as _**corpus, document, sentence, word, stop words, punctuation, stemming, and lemmatization.**_) commonly found in NLP. Utilizes the _**Natural Language Toolkit(NLTK)**_. Also used X, formerly known as Twitter, to parse and count number of documents, sentences, words, stopwords, punctuations, special words, unique words, stems, and lemmas. Also includes few Statistics problems.
* **Lab 2** dives into and implements _**Byte Pair Encoding(BPE)**_, showcasing representation of word segmentation strategies. Uses _**Tokenizers**_. Starts with a base of contractions (I'm, you're, etc.) and expands them (I, am, you, are, etc.), then utilizes the BPE algorithm to expand the contractiosn in a sentence. Also dives into _**Regular Expressions**_, using **_beautifulsoup4_** to process HTML. Initial use of a chatbot from NLTK, providing answers to specific questions by parsing the HTML of a website. Also showcases implementation of a _**Spell Checker**_ for phones, and _**Weighted Minimum Edit Distance**_, utilizing delete, insertion, and substitution costs.
* **Lab 3** discusses n-gram words and models, diving into symbols needed to prefix or suffix a sentence. _**Add-k smoothing**_, which pretends to see each word k more times to prevent zero probabilities. For example, providing the bigram counts and probabilities in a sentence given a corpus split by '<s>'. Calculates into _**perplexity**_, and also generates texts using unigrams, bigrams, and trigrams based off Gutenberg and punkt.
* **Lab 4** dives into _**Naive Bayes classifiers**_ and _**vocabulary**_, show likelihoods for each word being apart of a class. Showcases Naive Bayes classifier with _**binary bag of words**_ to classify movies from a dataset on sentiment analysis of imdb movie reviews, utilizing _**CountVectorizer**_. Also showcase it with a Reuters dataset for multiclass classifcation.
* **Lab 5** dives into **_Deep Learning(DL)_** networks in conjunction with NLP models. Split dataset into test and training set, computing the loss and accuracy for each. Create a DL classifier for sentiment analysis with imdb reviews. Run a comparison in accuracy between Naive Bayes, Logistic Regression, and DL. Also performs document classifcation utilizing _**SentenceTransformers**_.
* **Lab 7** dives into _**word embeddings**_, training _**GloVe**_ embeddings on imdb reviews, creating co-occurence matrices and using a log-bilinear regression model to get the relationships between words. Dives into _**Convolution based NLP**_, building an LLM to predict the next word, and implement and train a network that will predict the sentiment of the sentence. Dives into _**Feedforward Neural Networks**_ and _**Recurrent Neural Networks**_. Done in _**Tensorflow**_.  
* **Lab 8** reimplements GloVe in pytorch. Follows from Lab 7, except all done in torch. Utilizes a dataset from Kaggle, spotify million song dataset.
* **Lab 9** utilizes Huggingface to create a _**zero-shot classifier**_. Evaluates tweets using transformers' pipeline function for zero-shot classification. Utilizes _**datasets**_ functions to create train, test, and validation splits utilizing features text and label. Perform sentence classifiers using _**BERT**_. Implements a ChatBot using _**LangChain**_, using OpenAI's gemma:2b model to act as a ChatBot. Utilizes _**Ollama**_ to implement a _**Retrieval-Augmented Generation(RAG)**_ search on academic papers about RAG, using phi3 model to read the papers and explain what RAG is to the user.
* **Final Project** utilizes fireworks-ai and OpenAI to parse a drive folder with a background and reference folder containing documents. Utilizes embeddings for all the pages, then generate a response using OpenAI's API. Retrieves passages, asking the ChatBot to answer questions related to machine learning techniques. 
